{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789-Washington.txt\n",
      "char 8619\n",
      "words 1538\n",
      "sents 24\n",
      "1793-Washington.txt\n",
      "char 791\n",
      "words 147\n",
      "sents 4\n",
      "1797-Adams.txt\n",
      "char 13877\n",
      "words 2585\n",
      "sents 37\n",
      "1801-Jefferson.txt\n",
      "char 10137\n",
      "words 1935\n",
      "sents 42\n",
      "1805-Jefferson.txt\n",
      "char 12908\n",
      "words 2384\n",
      "sents 45\n",
      "1809-Madison.txt\n",
      "char 7001\n",
      "words 1265\n",
      "sents 21\n",
      "1813-Madison.txt\n",
      "char 7157\n",
      "words 1304\n",
      "sents 33\n",
      "1817-Monroe.txt\n",
      "char 19887\n",
      "words 3693\n",
      "sents 122\n",
      "1821-Monroe.txt\n",
      "char 26326\n",
      "words 4909\n",
      "sents 129\n",
      "1825-Adams.txt\n",
      "char 17741\n",
      "words 3150\n",
      "sents 74\n",
      "1829-Jackson.txt\n",
      "char 6817\n",
      "words 1208\n",
      "sents 25\n",
      "1833-Jackson.txt\n",
      "char 7058\n",
      "words 1267\n",
      "sents 30\n",
      "1837-VanBuren.txt\n",
      "char 23417\n",
      "words 4171\n",
      "sents 95\n",
      "1841-Harrison.txt\n",
      "char 49700\n",
      "words 9165\n",
      "sents 210\n",
      "1845-Polk.txt\n",
      "char 28716\n",
      "words 5196\n",
      "sents 153\n",
      "1849-Taylor.txt\n",
      "char 6605\n",
      "words 1182\n",
      "sents 22\n",
      "1853-Pierce.txt\n",
      "char 20081\n",
      "words 3657\n",
      "sents 104\n",
      "1857-Buchanan.txt\n",
      "char 16815\n",
      "words 3098\n",
      "sents 89\n",
      "1861-Lincoln.txt\n",
      "char 21017\n",
      "words 4005\n",
      "sents 138\n",
      "1865-Lincoln.txt\n",
      "char 3926\n",
      "words 785\n",
      "sents 27\n",
      "1869-Grant.txt\n",
      "char 6503\n",
      "words 1239\n",
      "sents 41\n",
      "1873-Grant.txt\n",
      "char 7734\n",
      "words 1478\n",
      "sents 44\n",
      "1877-Hayes.txt\n",
      "char 14938\n",
      "words 2724\n",
      "sents 59\n",
      "1881-Garfield.txt\n",
      "char 17767\n",
      "words 3239\n",
      "sents 112\n",
      "1885-Cleveland.txt\n",
      "char 10145\n",
      "words 1828\n",
      "sents 44\n",
      "1889-Harrison.txt\n",
      "char 26179\n",
      "words 4750\n",
      "sents 157\n",
      "1893-Cleveland.txt\n",
      "char 12349\n",
      "words 2153\n",
      "sents 58\n",
      "1897-McKinley.txt\n",
      "char 23659\n",
      "words 4371\n",
      "sents 130\n",
      "1901-McKinley.txt\n",
      "char 13408\n",
      "words 2450\n",
      "sents 100\n",
      "1905-Roosevelt.txt\n",
      "char 5568\n",
      "words 1091\n",
      "sents 33\n",
      "1909-Taft.txt\n",
      "char 32164\n",
      "words 5846\n",
      "sents 159\n",
      "1913-Wilson.txt\n",
      "char 9563\n",
      "words 1905\n",
      "sents 68\n",
      "1917-Wilson.txt\n",
      "char 8395\n",
      "words 1656\n",
      "sents 60\n",
      "1921-Harding.txt\n",
      "char 20298\n",
      "words 3756\n",
      "sents 149\n",
      "1925-Coolidge.txt\n",
      "char 23949\n",
      "words 4442\n",
      "sents 197\n",
      "1929-Hoover.txt\n",
      "char 21764\n",
      "words 3890\n",
      "sents 158\n",
      "1933-Roosevelt.txt\n",
      "char 10903\n",
      "words 2063\n",
      "sents 85\n",
      "1937-Roosevelt.txt\n",
      "char 10607\n",
      "words 2019\n",
      "sents 96\n",
      "1941-Roosevelt.txt\n",
      "char 7571\n",
      "words 1536\n",
      "sents 68\n",
      "1945-Roosevelt.txt\n",
      "char 3039\n",
      "words 637\n",
      "sents 26\n",
      "1949-Truman.txt\n",
      "char 13679\n",
      "words 2528\n",
      "sents 116\n",
      "1953-Eisenhower.txt\n",
      "char 13955\n",
      "words 2775\n",
      "sents 123\n",
      "1957-Eisenhower.txt\n",
      "char 9190\n",
      "words 1917\n",
      "sents 92\n",
      "1961-Kennedy.txt\n",
      "char 7618\n",
      "words 1546\n",
      "sents 52\n",
      "1965-Johnson.txt\n",
      "char 8193\n",
      "words 1715\n",
      "sents 94\n",
      "1969-Nixon.txt\n",
      "char 11624\n",
      "words 2425\n",
      "sents 106\n",
      "1973-Nixon.txt\n",
      "char 9991\n",
      "words 2028\n",
      "sents 69\n",
      "1977-Carter.txt\n",
      "char 6873\n",
      "words 1380\n",
      "sents 53\n",
      "1981-Reagan.txt\n",
      "char 13735\n",
      "words 2801\n",
      "sents 127\n",
      "1985-Reagan.txt\n",
      "char 14561\n",
      "words 2946\n",
      "sents 126\n",
      "1989-Bush.txt\n",
      "char 12523\n",
      "words 2713\n",
      "sents 145\n",
      "1993-Clinton.txt\n",
      "char 9114\n",
      "words 1855\n",
      "sents 81\n",
      "1997-Clinton.txt\n",
      "char 12250\n",
      "words 2462\n",
      "sents 112\n",
      "2001-Bush.txt\n",
      "char 9053\n",
      "words 1825\n",
      "sents 97\n",
      "2005-Bush.txt\n",
      "char 12018\n",
      "words 2376\n",
      "sents 95\n",
      "2009-Obama.txt\n",
      "char 13439\n",
      "words 2726\n",
      "sents 112\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.fileids()\n",
    "\n",
    "for fileid in inaugural.fileids():\n",
    "    print(fileid)\n",
    "    print(\"char\",len(inaugural.raw(fileid)))\n",
    "    print(\"words\",len(inaugural.words(fileid)))\n",
    "    print(\"sents\",len(inaugural.sents(fileid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-1-1: Divide it into two datasets: preWWI (till 1913) and postWWI (after 1917)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preWWI_lst = [] #Create list to collect preWWI (till 1913) fileids\n",
    "postWWI_lst =[] #Create list to collect postWWI (after 1917) fileids\n",
    "\n",
    "for fileid in inaugural.fileids():\n",
    "    if int(fileid[0:4])<=1913: #Collect the fileids before or on 1913\n",
    "        preWWI_lst.append(fileid)\n",
    "    elif int(fileid[0:4]) >= 1917: #Collect the fileids after on 1917\n",
    "        postWWI_lst.append(fileid)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part-1-2: Normalize the text for each dataset by down-casing all words (but no stemming or lemmatization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89718\n",
      "38423\n"
     ]
    }
   ],
   "source": [
    "pre_words_lst = []\n",
    "for i in range(len(preWWI_lst)):\n",
    "    pre_words_lst.append(list(inaugural.words(preWWI_lst[i])))\n",
    "    \n",
    "pre_words_lst2 = []\n",
    "for i in range(len(pre_words_lst)):\n",
    "    pre_words_lst2 = pre_words_lst2 + pre_words_lst[i]    \n",
    "pre_words_lst2 = [word.lower() for word in pre_words_lst2]\n",
    "print(len(pre_words_lst2))\n",
    "\n",
    "pre_words_lst3 = []\n",
    "for word in pre_words_lst2:\n",
    "    if word in stopwords.words('english') or len(word)<=2:\n",
    "        pass\n",
    "    else:\n",
    "        pre_words_lst3.append(word)\n",
    "print(len(pre_words_lst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56017\n",
      "23719\n"
     ]
    }
   ],
   "source": [
    "post_words_lst = []\n",
    "for i in range(len(postWWI_lst)):\n",
    "    post_words_lst.append(list(inaugural.words(postWWI_lst[i])))\n",
    "    \n",
    "post_words_lst2 = []\n",
    "for i in range(len(post_words_lst)):\n",
    "    post_words_lst2 = post_words_lst2 + post_words_lst[i]    \n",
    "post_words_lst2 = [word.lower() for word in post_words_lst2]\n",
    "print(len(post_words_lst2))\n",
    "\n",
    "post_words_lst3 = []\n",
    "for word in post_words_lst2:\n",
    "    if word in stopwords.words('english') or len(word)<=2:\n",
    "        pass\n",
    "    else:\n",
    "        post_words_lst3.append(word)\n",
    "print(len(post_words_lst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pre_words_lst2\n",
    "prev_word = \"\"\n",
    "pre_Bigrams_dict={}\n",
    "for word in tokens:\n",
    "    bigram = [prev_word , word]\n",
    "    if bigram in pre_Bigrams_dict:\n",
    "        pre_Bigrams_dict[set(bigram)] += 1\n",
    "    else:\n",
    "        pre_Bigrams_dict[bigram] = 1\n",
    "    \n",
    "    prev_word = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = post_words_lst2\n",
    "prev_word =\"\"\n",
    "post_Bigrams_dict={}\n",
    "for word in tokens:\n",
    "    bigram= prev_word + \" \" + word\n",
    "    if bigram in pre_Bigrams_dict:\n",
    "        if bigram in post_Bigrams_dict:\n",
    "            post_Bigrams_dict[bigram] +=1\n",
    "        else:\n",
    "            post_Bigrams_dict[bigram] =1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part-1-3: For each dataset, compute the frequency distribution (i.e., probability) of unigrams and bigrams.  Then compute the following and write two output files -- one for unigrams and one for bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the frequency of words appearing preWWI dataset into dictionary\n",
    "fduni = nltk.FreqDist(pre_words_lst3)\n",
    "#Convert words dictionary into dataframe\n",
    "pre_df = pd.DataFrame(list(zip(fduni.keys(), fduni.values())), columns =[\"tokens\", \"freq\"])\n",
    "#Sort the table by frequency values\n",
    "pre_df = pre_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "#Calculate the probabilities of the words appearing in the preWWI dataset and include the column in the table\n",
    "pre_df[\"pre_prob\"] = [round(freq / len(pre_words_lst2),7) for freq in pre_df.freq]\n",
    "#Sort the table by probailities value\n",
    "pre_df = pre_df.sort_values(by = [\"freq\"], ascending = False)\n",
    "#pre_df = pre_df.iloc[0:355]\n",
    "#for i in range(0, pre_df.shape[0]):\n",
    " #   print(\"%d%15s %0.7f\"%(i,pre_df.iloc[i][\"tokens\"], pre_df.iloc[i][\"pre_prob\"]))\n",
    "    \n",
    "pre_df=pre_df.iloc[0:200].sort_values(by=[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the frequency of words appearing postWWI dataset into dictionary\n",
    "fduni = nltk.FreqDist(post_words_lst2)\n",
    "#Convert words dictionary into dataframe\n",
    "post_df = pd.DataFrame(list(zip(fduni.keys(), fduni.values())), columns =[\"tokens\", \"freq\"])\n",
    "#Sort the table by frequency values\n",
    "post_df = post_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "#Calculate the probabilities of the words appearing in the preWWI dataset and include the column in the table\n",
    "post_df[\"post_prob\"] = [round(freq / len(post_words_lst2),7) for freq in post_df.freq]\n",
    "#Sort the table by probailities value\n",
    "post_df = post_df.sort_values(by = [\"freq\"], ascending = False)\n",
    "\n",
    "#post_df = post_df.iloc[0:355]\n",
    "#post_df\n",
    "#for i in range(0, post_df.shape[0]):\n",
    " #   print(\"%d%15s %0.7f\"%(i,post_df.iloc[i][\"tokens\"], post_df.iloc[i][\"post_prob\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the PreWWI and PostWWI dataframes (already sorted by probabilities in decending order) \n",
    "#in \"outer\" format which means the values show nan if NO tokens matches when merging the table\n",
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\", how=\"outer\")\n",
    "pre_post_df.dropna(axis=0, how=\"any\", inplace=True)\n",
    "#pre_post_df=pre_post_df.iloc[0:200]\n",
    "\n",
    "for ind in pre_post_df.index:\n",
    "    if len(pre_post_df.loc[ind].tokens)<=2 or (pre_post_df.loc[ind].tokens in stopwords.words()):\n",
    "        pre_post_df.drop(index=ind, axis=0,inplace=True)\n",
    "\n",
    "#pre_post_df.iloc[0].tokens\n",
    "#pre_post_df.drop(index=1, axis=0, inplace=True)\n",
    "pre_post_df = pre_post_df.sort_values(by=[\"tokens\"])#.to_csv(\"unigrams.txt\")\n",
    "pre_post_df.columns = [\"unigram\",\"pre_freq\",\"pre-WWI\",\"post_freq\",\"post-WWI\" ]\n",
    "with open(\"unigrams.txt\", \"w\") as f:\n",
    "    f.write(\"{0:<20}{1:<10}{2}\\n\".format(\"Unigram\",\"pre-WWI\",\"post-WWI\" ))\n",
    "    f.write(\"%s\\n\" %(\"=\"*38))\n",
    "    for i in range(0,pre_post_df.shape[0]):\n",
    "        f.write(\"{0:<17}{1: 0.7f} {2: 0.7f}\\n\".format(pre_post_df.iloc[i].unigram,\n",
    "                                                         pre_post_df.iloc[i][\"pre-WWI\"], pre_post_df.iloc[i][\"post-WWI\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(41780, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_bigrams = list(nltk.bigrams(pre_words_lst2))\n",
    "print(len(pre_bigrams))\n",
    "pre_fdbi = nltk.FreqDist(pre_bigrams)\n",
    "pre_df = pd.DataFrame(list(zip(pre_fdbi.keys(), pre_fdbi.values())), columns =[\"tokens\", \"freq\"])\n",
    "pre_df = pre_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "pre_df[\"pre_prob\"] = [round(freq / len(pre_bigrams),7) for freq in pre_df.freq]\n",
    "pre_df = pre_df.sort_values(by = [\"pre_prob\"], ascending = False)\n",
    "pre_df.shape\n",
    "#pre_df.to_csv(\"bigram.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27983, 3)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_bigrams = list(nltk.bigrams(post_words_lst2))\n",
    "print(len(post_bigrams))\n",
    "post_fdbi = nltk.FreqDist(post_bigrams)\n",
    "post_df = pd.DataFrame(list(zip(post_fdbi.keys(), post_fdbi.values())), columns =[\"tokens\", \"freq\"])\n",
    "post_df = post_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "post_df[\"post_prob\"] = [round(freq / len(post_bigrams),7) for freq in post_df.freq]\n",
    "post_df = post_df.sort_values(by = [\"post_prob\"], ascending = False)\n",
    "post_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_lst=[]\n",
    "v1_lst=[]\n",
    "v2_lst=[]\n",
    "for k1, v1 in pre_fdbi.items():\n",
    "    for k2, v2 in post_fdbi.items():\n",
    "        if k1[0] ==k2[0] and k1[1] == k2[1]:\n",
    "            k_lst.append(k1)\n",
    "            v1_lst.append(v1)\n",
    "            v2_lst.append(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'citizens'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.bigrams[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(list(zip(k_lst,v1_lst,v2_lst)), columns=[\"bigrams\",\"pre\",\"post\"])\n",
    "for i in df.index:\n",
    "    if df.bigrams[i][0] in stopwords.words() or df.bigrams[i][1] in stopwords.words():\n",
    "        df.drop(index=i, inplace=True)\n",
    "    elif len(df.bigrams[i][0])<=2 or len(df.bigrams[i][1])<=2:\n",
    "        df.drop(index=i, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>pre</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(united, states)</td>\n",
       "      <td>125</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>(fellow, citizens)</td>\n",
       "      <td>53</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>(american, people)</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>(federal, government)</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3280</th>\n",
       "      <td>(years, ago)</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>(foreign, nations)</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>(every, citizen)</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>(four, years)</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>(public, money)</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>(free, government)</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2773</th>\n",
       "      <td>(best, interests)</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>(political, parties)</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>(beloved, country)</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>(free, people)</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306</th>\n",
       "      <td>(almighty, god)</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>(religious, liberty)</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6289</th>\n",
       "      <td>(national, life)</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>(several, states)</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409</th>\n",
       "      <td>(national, government)</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>(every, part)</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>(new, states)</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>(social, order)</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>(called, upon)</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>(great, nation)</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>(public, opinion)</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>(years, since)</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>(national, debt)</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>(public, servants)</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>(foreign, countries)</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>(good, faith)</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>(constitutional, liberty)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>(high, moral)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4701</th>\n",
       "      <td>(people, depend)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>(world, may)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>(confidence, upon)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321</th>\n",
       "      <td>(full, share)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4324</th>\n",
       "      <td>(personal, liberty)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343</th>\n",
       "      <td>(first, came)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4410</th>\n",
       "      <td>(great, good)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>(past, experience)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>(well, understood)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>(treasury, department)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4585</th>\n",
       "      <td>(great, principles)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>(one, might)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>(constitutional, authority)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4825</th>\n",
       "      <td>(human, family)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5320</th>\n",
       "      <td>(must, always)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>(much, disturbed)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>(last, week)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4936</th>\n",
       "      <td>(country, must)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>(common, destiny)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>(people, whose)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>(heavy, burdens)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5077</th>\n",
       "      <td>(one, among)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>(might, say)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>(shall, receive)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>(limited, period)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5224</th>\n",
       "      <td>(kept, pace)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>(great, conflict)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7769</th>\n",
       "      <td>(new, age)</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bigrams  pre  post\n",
       "147              (united, states)  125    27\n",
       "112            (fellow, citizens)   53    23\n",
       "360            (american, people)   22    17\n",
       "3448        (federal, government)   19    13\n",
       "3280                 (years, ago)   14    11\n",
       "621            (foreign, nations)   14     1\n",
       "2465             (every, citizen)   13     5\n",
       "3225                (four, years)   13    13\n",
       "2731              (public, money)   10     1\n",
       "228            (free, government)   10     1\n",
       "2773            (best, interests)    9     1\n",
       "3431         (political, parties)    8     3\n",
       "1175           (beloved, country)    8     1\n",
       "2355               (free, people)    8     4\n",
       "3306              (almighty, god)    8     7\n",
       "1908         (religious, liberty)    8     1\n",
       "6289             (national, life)    8     4\n",
       "1840            (several, states)    7     1\n",
       "2409       (national, government)    7     4\n",
       "2869                (every, part)    6     1\n",
       "2405                (new, states)    6     1\n",
       "4011              (social, order)    6     1\n",
       "495                (called, upon)    6     3\n",
       "2854              (great, nation)    6     7\n",
       "1020            (public, opinion)    5     2\n",
       "3226               (years, since)    5     1\n",
       "2718             (national, debt)    5     1\n",
       "1842           (public, servants)    5     1\n",
       "5117         (foreign, countries)    5     1\n",
       "5999                (good, faith)    5     5\n",
       "...                           ...  ...   ...\n",
       "5389    (constitutional, liberty)    1     1\n",
       "5353                (high, moral)    1     1\n",
       "4701             (people, depend)    1     1\n",
       "4222                 (world, may)    1     2\n",
       "4249           (confidence, upon)    1     1\n",
       "4321                (full, share)    1     2\n",
       "4324          (personal, liberty)    1     1\n",
       "4343                (first, came)    1     2\n",
       "4410                (great, good)    1     2\n",
       "4450           (past, experience)    1     1\n",
       "4468           (well, understood)    1     1\n",
       "4516       (treasury, department)    1     1\n",
       "4585          (great, principles)    1     1\n",
       "4632                 (one, might)    1     1\n",
       "4681  (constitutional, authority)    1     1\n",
       "4825              (human, family)    1     1\n",
       "5320               (must, always)    1     2\n",
       "4839            (much, disturbed)    1     1\n",
       "4880                 (last, week)    1     1\n",
       "4936              (country, must)    1     1\n",
       "4953            (common, destiny)    1     1\n",
       "4971              (people, whose)    1     2\n",
       "4986             (heavy, burdens)    1     1\n",
       "5077                 (one, among)    1     1\n",
       "5102                 (might, say)    1     3\n",
       "5168             (shall, receive)    1     1\n",
       "5208            (limited, period)    1     1\n",
       "5224                 (kept, pace)    1     1\n",
       "5278            (great, conflict)    1     1\n",
       "7769                   (new, age)    1     4\n",
       "\n",
       "[381 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"pre\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens           0\n",
       "freq_x       20197\n",
       "pre_prob     20197\n",
       "freq_y       33994\n",
       "post_prob    33994\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge the PreWWI and PostWWI dataframes (already sorted by probabilities in decending order) \n",
    "#in \"outer\" format which means the values show nan if NO tokens matches when merging the table\n",
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\", how=\"outer\")\n",
    "pre_post_df = pre_post_df[pre_post_df.isnull() == False]\n",
    "\n",
    "pre_post_df.set_index([\"tokens\"], inplace=True)\n",
    "for ind in pre_post_df.index:\n",
    "    if (len(ind[0])<=2) or (len(ind[1])<=2)or (ind[0] in stopwords.words()) or (ind[1] in stopwords.words()):\n",
    "        pre_post_df.drop(index=ind, inplace=True)\n",
    "\n",
    "pre_post_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>freq_x</th>\n",
       "      <th>pre_prob</th>\n",
       "      <th>freq_y</th>\n",
       "      <th>post_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tokens, freq_x, pre_prob, freq_y, post_prob]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_post_df = pre_post_df.dropna(axis=0,how=\"any\")\n",
    "#pre_post_df[pre_post_df.tokens == \"(of, the)\"]\n",
    "pre_post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the PreWWI and PostWWI dataframes (already sorted by probabilities in decending order) \n",
    "#in \"outer\" format which means the values show nan if NO tokens matches when merging the table\n",
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\", how=\"outer\")\n",
    "\n",
    "pre_post_df.set_index([\"tokens\"], inplace=True)\n",
    "for ind in pre_post_df.index:\n",
    "    if (len(ind[0])<=2) or (len(ind[1])<=2)or (ind[0] in stopwords.words()) or (ind[1] in stopwords.words()):\n",
    "        pre_post_df.drop(index=ind, inplace=True)\n",
    "\n",
    "#Drop all the nan rows after merging tables\n",
    "pre_post_df.dropna(axis=0,how=\"any\", inplace=True)\n",
    "#Filter out null cell after merging\n",
    "pre_post_df=pre_post_df[pre_post_df.isnull() == False]\n",
    "#Reset the index of table\n",
    "pre_post_df.reset_index(drop=False, inplace=True)\n",
    "#Drop Frequency column\n",
    "pre_post_df.drop(columns=[\"freq_x\",\"freq_y\"], inplace=True)\n",
    "#Reset column name\n",
    "pre_post_df.columns = ['bigrams', \"pre-WWI\"  ,  \"post-WWI \"]\n",
    "#pre_post_df.to_csv(\"test_uni.csv\")\n",
    "#Sort out the most frequency  unigrams in union of Pre-WWI and Post-WWI\n",
    "pre_post_df = pre_post_df.sort_values(by=[\"bigrams\"])\n",
    "pre_post_df.to_csv(\"bigram.txt\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Barplot the top 50 frequency of bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pre-WWI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pre-WWI'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-36fa2376e590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#pre_post_df.drop(columns = [\"freq_x\",\"freq_y\"], inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Extract the top 1000 Frequency rows of the table and sort the table by the column \"tokens\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpre_post_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_post_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pre-WWI\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[1;32m   3617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3618\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3619\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3620\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   2333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2335\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pre-WWI'"
     ]
    }
   ],
   "source": [
    "#Merge the PreWWI and PostWWI dataframes (already sorted by probabilities in decending order) \n",
    "#in \"outer\" format which means the values show nan if NO tokens matches when merging the table\n",
    "#pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\", how=\"outer\")\n",
    "#Drop all the nan rows after merging tables\n",
    "#pre_post_df.dropna(axis=0, how=\"any\", inplace= True)\n",
    "#Drop the frequency columns\n",
    "#pre_post_df.drop(columns = [\"freq_x\",\"freq_y\"], inplace=True)\n",
    "#Extract the top 1000 Frequency rows of the table and sort the table by the column \"tokens\"\n",
    "pre_post_df = pre_post_df[0:50].sort_values(by =[\"pre-WWI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'bigrams'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d91c5f4ac497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpre_post_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_post_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'barh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Probability of Common Bigrams\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Probability\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'bigrams'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f6b5240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(30,20))\n",
    "ax=fig.add_subplot(111)\n",
    "pre_post_df.plot(x=pre_post_df.bigrams,kind='barh',legend=False, ax=ax)\n",
    "plt.title(\"Probability of Common Bigrams\", size=25 )\n",
    "plt.xlabel(\"Probability\", size=23)\n",
    "plt.ylabel(\"Bigrams\", size=23)\n",
    "plt.tick_params(axis='x', labelsize=18)\n",
    "plt.tick_params(axis='y', labelsize=18)\n",
    "plt.legend(loc='center right', fontsize=\"x-large\")\n",
    "fig.savefig('Top_50_Bigram_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cfdist(text, size=3):\n",
    "    \"\"\"\n",
    "    Extract unigrams and two-word tuples in the windows and create a \n",
    "    FrequencyDist dictionary for both (and returns them in a list)\n",
    "    \"\"\"\n",
    "    unigrams = []\n",
    "    tuples = []\n",
    "    # Scan over windows of the appropriate size.\n",
    "    for center in range(size, len(text)-size):\n",
    "        # enter the coocurrence (center word and each of all other words) in the dictionary\n",
    "        wunis = set()\n",
    "        wtuples = set() # for tuples in this context; set is to count only once\n",
    "        thisword = text[center]\n",
    "        \n",
    "        # iterate though the test of the window\n",
    "        for i in range(1, size+1): # i starts from 1 (center +/- i)\n",
    "            nextleft = text[center-i]\n",
    "            nextright = text[center+i]\n",
    "            # add them next word in this window's unigram set\n",
    "            wunis.add(nextleft)\n",
    "            wunis.add(nextright)\n",
    "            # create the next left tuple\n",
    "            if not thisword == nextleft:\n",
    "                if thisword < nextleft:\n",
    "                    tup = (thisword,nextleft)\n",
    "                else:\n",
    "                    tup = (nextleft,thisword)\n",
    "                # and add it in this window's tuple set\n",
    "                wtuples.add(tup) #\n",
    "            # create the next right tuple\n",
    "            if not thisword == nextright:\n",
    "                if thisword < nextright:\n",
    "                    tup = (thisword,nextright)\n",
    "                else:\n",
    "                    tup = (nextright,thisword)\n",
    "                # and add it in this window's tuple set\n",
    "                wtuples.add(tup) #\n",
    "        \n",
    "        # add all unigrams in the text tuples list\n",
    "        for wuni in wunis:\n",
    "            unigrams.append(wuni)\n",
    "        # add all tuples in the text tuples list\n",
    "        for wtup in wtuples:\n",
    "            tuples.append(wtup)\n",
    "            \n",
    "    # create a frequency dictionary from unigrams and tuples\n",
    "    ufd = nltk.FreqDist(unigrams)\n",
    "    cfd = nltk.FreqDist(tuples)\n",
    "    # and return the dictionaries in a list\n",
    "    return [ufd, cfd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"These approaches are less performant, but they will provide correct behavior. They will be much less performant than copy and update or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they do respect the order of precedence (latter dicts have precedence)\"\n",
    "from nltk import word_tokenize\n",
    "text = word_tokenize(text)\n",
    "ufd, cfd = make_cfdist(text, size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('(', 'dicts'): 2,\n",
       "          ('(', 'have'): 1,\n",
       "          ('(', 'latter'): 2,\n",
       "          ('(', 'of'): 2,\n",
       "          ('(', 'order'): 2,\n",
       "          ('(', 'precedence'): 2,\n",
       "          (')', 'dicts'): 1,\n",
       "          (',', 'abstraction'): 2,\n",
       "          (',', 'are'): 1,\n",
       "          (',', 'but'): 4,\n",
       "          (',', 'do'): 2,\n",
       "          (',', 'less'): 2,\n",
       "          (',', 'level'): 2,\n",
       "          (',', 'of'): 2,\n",
       "          (',', 'performant'): 2,\n",
       "          (',', 'they'): 4,\n",
       "          (',', 'will'): 2,\n",
       "          ('.', 'They'): 2,\n",
       "          ('.', 'be'): 2,\n",
       "          ('.', 'behavior'): 2,\n",
       "          ('.', 'correct'): 2,\n",
       "          ('.', 'provide'): 2,\n",
       "          ('.', 'will'): 2,\n",
       "          ('These', 'less'): 1,\n",
       "          ('They', 'be'): 2,\n",
       "          ('They', 'behavior'): 2,\n",
       "          ('They', 'correct'): 2,\n",
       "          ('They', 'much'): 2,\n",
       "          ('They', 'will'): 2,\n",
       "          ('a', 'at'): 2,\n",
       "          ('a', 'higher'): 2,\n",
       "          ('a', 'key-value'): 2,\n",
       "          ('a', 'level'): 2,\n",
       "          ('a', 'of'): 2,\n",
       "          ('a', 'pair'): 2,\n",
       "          ('abstraction', 'but'): 2,\n",
       "          ('abstraction', 'higher'): 2,\n",
       "          ('abstraction', 'level'): 2,\n",
       "          ('abstraction', 'of'): 2,\n",
       "          ('abstraction', 'they'): 2,\n",
       "          ('and', 'copy'): 2,\n",
       "          ('and', 'or'): 2,\n",
       "          ('and', 'performant'): 2,\n",
       "          ('and', 'than'): 2,\n",
       "          ('and', 'the'): 2,\n",
       "          ('and', 'update'): 2,\n",
       "          ('approaches', 'less'): 1,\n",
       "          ('approaches', 'performant'): 1,\n",
       "          ('are', 'less'): 1,\n",
       "          ('are', 'performant'): 1,\n",
       "          ('at', 'each'): 2,\n",
       "          ('at', 'higher'): 2,\n",
       "          ('at', 'key-value'): 2,\n",
       "          ('at', 'level'): 2,\n",
       "          ('at', 'pair'): 2,\n",
       "          ('be', 'less'): 2,\n",
       "          ('be', 'much'): 2,\n",
       "          ('be', 'performant'): 2,\n",
       "          ('be', 'will'): 2,\n",
       "          ('because', 'iterate'): 2,\n",
       "          ('because', 'new'): 2,\n",
       "          ('because', 'the'): 2,\n",
       "          ('because', 'they'): 2,\n",
       "          ('because', 'through'): 2,\n",
       "          ('because', 'unpacking'): 2,\n",
       "          ('behavior', 'correct'): 2,\n",
       "          ('behavior', 'provide'): 2,\n",
       "          ('behavior', 'will'): 3,\n",
       "          ('but', 'do'): 2,\n",
       "          ('but', 'less'): 2,\n",
       "          ('but', 'of'): 2,\n",
       "          ('but', 'performant'): 2,\n",
       "          ('but', 'provide'): 2,\n",
       "          ('but', 'respect'): 2,\n",
       "          ('but', 'they'): 4,\n",
       "          ('but', 'will'): 2,\n",
       "          ('copy', 'less'): 2,\n",
       "          ('copy', 'or'): 2,\n",
       "          ('copy', 'performant'): 2,\n",
       "          ('copy', 'than'): 2,\n",
       "          ('copy', 'update'): 2,\n",
       "          ('correct', 'provide'): 2,\n",
       "          ('correct', 'they'): 2,\n",
       "          ('correct', 'will'): 2,\n",
       "          ('dicts', 'have'): 1,\n",
       "          ('dicts', 'latter'): 2,\n",
       "          ('dicts', 'precedence'): 2,\n",
       "          ('do', 'order'): 2,\n",
       "          ('do', 'respect'): 2,\n",
       "          ('do', 'the'): 2,\n",
       "          ('do', 'they'): 2,\n",
       "          ('each', 'iterate'): 2,\n",
       "          ('each', 'key-value'): 2,\n",
       "          ('each', 'pair'): 2,\n",
       "          ('each', 'they'): 2,\n",
       "          ('each', 'through'): 2,\n",
       "          ('have', 'latter'): 1,\n",
       "          ('higher', 'level'): 2,\n",
       "          ('higher', 'of'): 2,\n",
       "          ('higher', 'pair'): 2,\n",
       "          ('iterate', 'key-value'): 2,\n",
       "          ('iterate', 'they'): 2,\n",
       "          ('iterate', 'through'): 2,\n",
       "          ('iterate', 'unpacking'): 2,\n",
       "          ('key-value', 'pair'): 2,\n",
       "          ('key-value', 'through'): 2,\n",
       "          ('latter', 'of'): 2,\n",
       "          ('latter', 'precedence'): 2,\n",
       "          ('less', 'much'): 2,\n",
       "          ('less', 'performant'): 4,\n",
       "          ('less', 'than'): 2,\n",
       "          ('less', 'will'): 2,\n",
       "          ('level', 'of'): 2,\n",
       "          ('much', 'performant'): 2,\n",
       "          ('much', 'than'): 2,\n",
       "          ('much', 'will'): 2,\n",
       "          ('new', 'or'): 2,\n",
       "          ('new', 'the'): 2,\n",
       "          ('new', 'they'): 2,\n",
       "          ('new', 'unpacking'): 2,\n",
       "          ('new', 'update'): 2,\n",
       "          ('of', 'order'): 2,\n",
       "          ('of', 'precedence'): 2,\n",
       "          ('of', 'respect'): 2,\n",
       "          ('of', 'the'): 2,\n",
       "          ('or', 'the'): 2,\n",
       "          ('or', 'unpacking'): 2,\n",
       "          ('or', 'update'): 2,\n",
       "          ('order', 'precedence'): 2,\n",
       "          ('order', 'respect'): 2,\n",
       "          ('order', 'the'): 2,\n",
       "          ('pair', 'through'): 2,\n",
       "          ('performant', 'than'): 2,\n",
       "          ('performant', 'they'): 2,\n",
       "          ('precedence', 'the'): 2,\n",
       "          ('provide', 'they'): 2,\n",
       "          ('provide', 'will'): 2,\n",
       "          ('respect', 'the'): 2,\n",
       "          ('respect', 'they'): 2,\n",
       "          ('than', 'update'): 2,\n",
       "          ('the', 'they'): 2,\n",
       "          ('the', 'unpacking'): 2,\n",
       "          ('the', 'update'): 2,\n",
       "          ('they', 'through'): 2,\n",
       "          ('they', 'unpacking'): 2,\n",
       "          ('they', 'will'): 2})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_df.set_index(\"tokens\").join(post_df.set_index(\"tokens\"),)\n",
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\")\n",
    "pre_post_df.columns = [\"unigram\", \"pre-WWI\",\"post-WWI\" ]\n",
    "pre_post_df = pre_post_df.sort_values(by = [\"pre-WWI\"], ascending=False)\n",
    "pre_post_df = pre_post_df[0:200]\n",
    "pre_post_df.sort_values(by = [\"unigram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating the preWWI list to down-casing all words\n",
    "for i in range(len(preWWI_lst)):\n",
    "    preWWI_lst[i] = inaugural.raw(preWWI_lst[i]).lower() #Store the preWWI data (ALL words) to the same list preWWI_lst\n",
    "\n",
    "#Iterating the postWWI_lst list to down-casing all words\n",
    "for i in range(len(postWWI_lst)):\n",
    "    postWWI_lst[i] = inaugural.raw(postWWI_lst[i]).lower() #Store the postWWI data (ALL words) to the same list postWWI_lst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part-1-3: For each dataset, compute the frequency distribution (i.e., probability) of unigrams and bigrams.  Then compute the following and write two output files -- one for unigrams and one for bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/KevQuant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c69ce2ae1974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwords_lst3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_lst2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "words_lst=[]\n",
    "for i in range(len(preWWI_lst)):\n",
    "    words_lst.append( word_tokenize(preWWI_lst[i]))\n",
    "\n",
    "words_lst2=[]\n",
    "for i in range(len(words_lst)):\n",
    "    words_lst2 +=words_lst[i]\n",
    "\n",
    "words_lst3=[]\n",
    "for word in words_lst2:\n",
    "    if (len(word)<=2) or (word in stopwords.words()):\n",
    "        pass\n",
    "    else:\n",
    "        words_lst3.append(word)\n",
    "\n",
    "fduni = nltk.FreqDist(words_lst3)\n",
    "len(fduni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preWWI_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6769"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict = {}\n",
    "for words in preWWI_lst:\n",
    "    words = word_tokenize(words)\n",
    "    for word in words:\n",
    "        if word in stopwords.words() or len(word)<=2:\n",
    "            pass\n",
    "        else:\n",
    "            if word.strip() in words_dict.keys():\n",
    "                words_dict[word.strip()]+=1\n",
    "            else:\n",
    "                words_dict[word.strip()]=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6761"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict2 = {}\n",
    "for k,v in words_dict.items():\n",
    "    if v >= 200:\n",
    "        pass\n",
    "    else:\n",
    "        words_dict2[k] = v\n",
    "words_dict2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(words_dict2.keys(), words_dict2.values())), columns =[\"tokens\", \"freq\"])\n",
    "df.sort_values(by=[\"tokens\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-01ea01abaaf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreWWI_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreWWI_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreWWI_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreWWI_words\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mpreWWI_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreWWI_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfreq_uni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "preWWI_words=[]\n",
    "for i in range(len(preWWI_lst)):\n",
    "    preWWI_words = preWWI_words +preWWI_lst[i]\n",
    "words = word_tokenize(preWWI_words)\n",
    "freq_uni = nltk.FreqDist(words)\n",
    "df = pd.DataFrame(list(zip(freq_uni.keys(), freq_uni.values())), columns = [\"tokens\", \"freq\"])\n",
    "#df.drop(labels=[5])\n",
    "df.sort_values(by = [\"tokens\"])\n",
    "#df.drop(labels=[5])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preWWI_words = []\n",
    "for i in range(len(preWWI_lst)):\n",
    "    preWWI_words = preWWI_words  + [preWWI_lst[i]]\n",
    "\n",
    "len(preWWI_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488573"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt =0\n",
    "for words in preWWI_lst:\n",
    "    cnt += len(words)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abced']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = [[\"abced\"], [\"fuahf fiaha\"]]\n",
    "\n",
    "aa= aa[0]+aa[1]\n",
    "word_tokenize(aa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6936"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst=[]\n",
    "for i in range(len(preWWI_lst)):\n",
    "    words_lst.append( word_tokenize(preWWI_lst[i]))\n",
    "\n",
    "words_lst2=[]\n",
    "for i in range(len(words_lst)):\n",
    "    words_lst2 +=words_lst[i]\n",
    "\n",
    "fduni = nltk.FreqDist(words_lst2)\n",
    "len(fduni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89345"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt=1\n",
    "for words in words_lst:\n",
    "    cnt += len(words)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams: 145735\n",
      "bigram 145734\n"
     ]
    }
   ],
   "source": [
    "unigrams = inaugural.words()\n",
    "print(\"unigrams:\", len(unigrams))\n",
    "bigrams = list(nltk.bigrams(unigrams))\n",
    "print(\"bigram\",len(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fduni = nltk.FreqDist(unigrams)\n",
    "print(\"length of unigrams Dict\",len(fduni))\n",
    "\n",
    "for (w,c) in fduni.items():\n",
    "    print(w,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Check out the frequency of each word link to another word next to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('-', 7), ('citizens', 16), ('Citizens', 1)])\n"
     ]
    }
   ],
   "source": [
    "cfd =  nltk.ConditionalFreqDist(bigrams)\n",
    "print(cfd[\"Fellow\"].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Conditional Probabilities of \n",
    "##(freq(\"-\") + freq(\"Fellow\")) divided by (Total freq(\"fellow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2916666666666667"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"Fellow\"].freq(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
