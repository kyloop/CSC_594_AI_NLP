{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/KevQuant/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"inaugural\")\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789-Washington.txt\n",
      "char 8619\n",
      "words 1538\n",
      "sents 24\n",
      "1793-Washington.txt\n",
      "char 791\n",
      "words 147\n",
      "sents 4\n",
      "1797-Adams.txt\n",
      "char 13877\n",
      "words 2585\n",
      "sents 37\n",
      "1801-Jefferson.txt\n",
      "char 10137\n",
      "words 1935\n",
      "sents 42\n",
      "1805-Jefferson.txt\n",
      "char 12908\n",
      "words 2384\n",
      "sents 45\n",
      "1809-Madison.txt\n",
      "char 7001\n",
      "words 1265\n",
      "sents 21\n",
      "1813-Madison.txt\n",
      "char 7157\n",
      "words 1304\n",
      "sents 33\n",
      "1817-Monroe.txt\n",
      "char 19887\n",
      "words 3693\n",
      "sents 122\n",
      "1821-Monroe.txt\n",
      "char 26326\n",
      "words 4909\n",
      "sents 129\n",
      "1825-Adams.txt\n",
      "char 17741\n",
      "words 3150\n",
      "sents 74\n",
      "1829-Jackson.txt\n",
      "char 6817\n",
      "words 1208\n",
      "sents 25\n",
      "1833-Jackson.txt\n",
      "char 7058\n",
      "words 1267\n",
      "sents 30\n",
      "1837-VanBuren.txt\n",
      "char 23417\n",
      "words 4171\n",
      "sents 95\n",
      "1841-Harrison.txt\n",
      "char 49700\n",
      "words 9165\n",
      "sents 210\n",
      "1845-Polk.txt\n",
      "char 28716\n",
      "words 5196\n",
      "sents 153\n",
      "1849-Taylor.txt\n",
      "char 6605\n",
      "words 1182\n",
      "sents 22\n",
      "1853-Pierce.txt\n",
      "char 20081\n",
      "words 3657\n",
      "sents 104\n",
      "1857-Buchanan.txt\n",
      "char 16815\n",
      "words 3098\n",
      "sents 89\n",
      "1861-Lincoln.txt\n",
      "char 21017\n",
      "words 4005\n",
      "sents 138\n",
      "1865-Lincoln.txt\n",
      "char 3926\n",
      "words 785\n",
      "sents 27\n",
      "1869-Grant.txt\n",
      "char 6503\n",
      "words 1239\n",
      "sents 41\n",
      "1873-Grant.txt\n",
      "char 7734\n",
      "words 1478\n",
      "sents 44\n",
      "1877-Hayes.txt\n",
      "char 14938\n",
      "words 2724\n",
      "sents 59\n",
      "1881-Garfield.txt\n",
      "char 17767\n",
      "words 3239\n",
      "sents 112\n",
      "1885-Cleveland.txt\n",
      "char 10145\n",
      "words 1828\n",
      "sents 44\n",
      "1889-Harrison.txt\n",
      "char 26179\n",
      "words 4750\n",
      "sents 157\n",
      "1893-Cleveland.txt\n",
      "char 12349\n",
      "words 2153\n",
      "sents 58\n",
      "1897-McKinley.txt\n",
      "char 23659\n",
      "words 4371\n",
      "sents 130\n",
      "1901-McKinley.txt\n",
      "char 13408\n",
      "words 2450\n",
      "sents 100\n",
      "1905-Roosevelt.txt\n",
      "char 5568\n",
      "words 1091\n",
      "sents 33\n",
      "1909-Taft.txt\n",
      "char 32164\n",
      "words 5846\n",
      "sents 159\n",
      "1913-Wilson.txt\n",
      "char 9563\n",
      "words 1905\n",
      "sents 68\n",
      "1917-Wilson.txt\n",
      "char 8395\n",
      "words 1656\n",
      "sents 60\n",
      "1921-Harding.txt\n",
      "char 20298\n",
      "words 3756\n",
      "sents 149\n",
      "1925-Coolidge.txt\n",
      "char 23949\n",
      "words 4442\n",
      "sents 197\n",
      "1929-Hoover.txt\n",
      "char 21764\n",
      "words 3890\n",
      "sents 158\n",
      "1933-Roosevelt.txt\n",
      "char 10903\n",
      "words 2063\n",
      "sents 85\n",
      "1937-Roosevelt.txt\n",
      "char 10607\n",
      "words 2019\n",
      "sents 96\n",
      "1941-Roosevelt.txt\n",
      "char 7571\n",
      "words 1536\n",
      "sents 68\n",
      "1945-Roosevelt.txt\n",
      "char 3039\n",
      "words 637\n",
      "sents 26\n",
      "1949-Truman.txt\n",
      "char 13679\n",
      "words 2528\n",
      "sents 116\n",
      "1953-Eisenhower.txt\n",
      "char 13955\n",
      "words 2775\n",
      "sents 123\n",
      "1957-Eisenhower.txt\n",
      "char 9190\n",
      "words 1917\n",
      "sents 92\n",
      "1961-Kennedy.txt\n",
      "char 7618\n",
      "words 1546\n",
      "sents 52\n",
      "1965-Johnson.txt\n",
      "char 8193\n",
      "words 1715\n",
      "sents 94\n",
      "1969-Nixon.txt\n",
      "char 11624\n",
      "words 2425\n",
      "sents 106\n",
      "1973-Nixon.txt\n",
      "char 9991\n",
      "words 2028\n",
      "sents 69\n",
      "1977-Carter.txt\n",
      "char 6873\n",
      "words 1380\n",
      "sents 53\n",
      "1981-Reagan.txt\n",
      "char 13735\n",
      "words 2801\n",
      "sents 127\n",
      "1985-Reagan.txt\n",
      "char 14561\n",
      "words 2946\n",
      "sents 126\n",
      "1989-Bush.txt\n",
      "char 12523\n",
      "words 2713\n",
      "sents 145\n",
      "1993-Clinton.txt\n",
      "char 9114\n",
      "words 1855\n",
      "sents 81\n",
      "1997-Clinton.txt\n",
      "char 12250\n",
      "words 2462\n",
      "sents 112\n",
      "2001-Bush.txt\n",
      "char 9053\n",
      "words 1825\n",
      "sents 97\n",
      "2005-Bush.txt\n",
      "char 12018\n",
      "words 2376\n",
      "sents 95\n",
      "2009-Obama.txt\n",
      "char 13439\n",
      "words 2726\n",
      "sents 112\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.fileids()\n",
    "\n",
    "for fileid in inaugural.fileids():\n",
    "    print(fileid)\n",
    "    print(\"char\",len(inaugural.raw(fileid)))\n",
    "    print(\"words\",len(inaugural.words(fileid)))\n",
    "    print(\"sents\",len(inaugural.sents(fileid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-1-1: Divide it into two datasets: preWWI (till 1913) and postWWI (after 1917)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preWWI_lst = [] #Create list to collect preWWI (till 1913) fileids\n",
    "postWWI_lst =[] #Create list to collect postWWI (after 1917) fileids\n",
    "\n",
    "for fileid in inaugural.fileids():\n",
    "    if int(fileid[0:4])<=1913: #Collect the fileids before or on 1913\n",
    "        preWWI_lst.append(fileid)\n",
    "    elif int(fileid[0:4]) >1917: #Collect the fileids after on 1917\n",
    "        postWWI_lst.append(fileid)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part-1-2: Normalize the text for each dataset by down-casing all words (but no stemming or lemmatization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89718\n",
      "38423\n"
     ]
    }
   ],
   "source": [
    "pre_words_lst = []\n",
    "for i in range(len(preWWI_lst)):\n",
    "    pre_words_lst.append(list(inaugural.words(preWWI_lst[i])))\n",
    "    \n",
    "pre_words_lst2 = []\n",
    "for i in range(len(pre_words_lst)):\n",
    "    pre_words_lst2 = pre_words_lst2 + pre_words_lst[i]    \n",
    "pre_words_lst2 = [word.lower() for word in pre_words_lst2]\n",
    "print(len(pre_words_lst2))\n",
    "\n",
    "pre_words_lst3 = []\n",
    "for word in pre_words_lst2:\n",
    "    if word in stopwords.words('english') or len(word)<=2:\n",
    "        pass\n",
    "    else:\n",
    "        pre_words_lst3.append(word)\n",
    "print(len(pre_words_lst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54361\n",
      "23063\n"
     ]
    }
   ],
   "source": [
    "post_words_lst = []\n",
    "for i in range(len(postWWI_lst)):\n",
    "    post_words_lst.append(list(inaugural.words(postWWI_lst[i])))\n",
    "    \n",
    "post_words_lst2 = []\n",
    "for i in range(len(post_words_lst)):\n",
    "    post_words_lst2 = post_words_lst2 + post_words_lst[i]    \n",
    "post_words_lst2 = [word.lower() for word in post_words_lst2]\n",
    "print(len(post_words_lst2))\n",
    "\n",
    "post_words_lst3 = []\n",
    "for word in post_words_lst2:\n",
    "    if word in stopwords.words('english') or len(word)<=2:\n",
    "        pass\n",
    "    else:\n",
    "        post_words_lst3.append(word)\n",
    "print(len(post_words_lst3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part-1-3: For each dataset, compute the frequency distribution (i.e., probability) of unigrams and bigrams.  Then compute the following and write two output files -- one for unigrams and one for bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the frequency of words appearing preWWI dataset into dictionary\n",
    "fduni = nltk.FreqDist(pre_words_lst3)\n",
    "#Convert words dictionary into dataframe\n",
    "pre_df = pd.DataFrame(list(zip(fduni.keys(), fduni.values())), columns =[\"tokens\", \"freq\"])\n",
    "#Sort the table by frequency values\n",
    "pre_df = pre_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "#Calculate the probabilities of the words appearing in the preWWI dataset and include the column in the table\n",
    "pre_df[\"pre_prob\"] = [round(freq / len(pre_words_lst2),7) for freq in pre_df.freq]\n",
    "#Sort the table by probailities value\n",
    "pre_df = pre_df.sort_values(by = [\"pre_prob\"], ascending = False)\n",
    "#pre_df = pre_df.iloc[0:355]\n",
    "#for i in range(0, pre_df.shape[0]):\n",
    " #   print(\"%d%15s %0.7f\"%(i,pre_df.iloc[i][\"tokens\"], pre_df.iloc[i][\"pre_prob\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the frequency of words appearing postWWI dataset into dictionary\n",
    "fduni = nltk.FreqDist(post_words_lst3)\n",
    "#Convert words dictionary into dataframe\n",
    "post_df = pd.DataFrame(list(zip(fduni.keys(), fduni.values())), columns =[\"tokens\", \"freq\"])\n",
    "#Sort the table by frequency values\n",
    "post_df = post_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "#Calculate the probabilities of the words appearing in the preWWI dataset and include the column in the table\n",
    "post_df[\"post_prob\"] = [round(freq / len(post_words_lst2),7) for freq in post_df.freq]\n",
    "#Sort the table by probailities value\n",
    "post_df = post_df.sort_values(by = [\"post_prob\"], ascending = False)\n",
    "\n",
    "#post_df = post_df.iloc[0:355]\n",
    "#post_df\n",
    "#for i in range(0, post_df.shape[0]):\n",
    " #   print(\"%d%15s %0.7f\"%(i,post_df.iloc[i][\"tokens\"], post_df.iloc[i][\"post_prob\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>pre_prob</th>\n",
       "      <th>post_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>action</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>administration</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>affairs</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>aid</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>also</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>always</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>american</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.001398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>among</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>another</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>authority</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>become</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>best</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>better</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>blessings</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>business</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>called</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>cause</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>character</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>circumstances</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>citizen</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>citizens</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.001159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>civil</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>commerce</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>common</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>condition</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>congress</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>constitution</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.000276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>constitutional</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>control</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>since</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>spirit</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.000975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>state</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>states</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.000773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>still</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>subject</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>support</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>system</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>take</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>territory</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>therefore</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>time</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.001968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>trade</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>true</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>trust</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>union</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>united</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.000920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>upon</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.001545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>war</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>well</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.000662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>whether</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>whole</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>whose</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>wisdom</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>within</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>without</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>world</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.004838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>would</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>years</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.000993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>yet</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tokens  pre_prob  post_prob\n",
       "176          action  0.000357   0.000497\n",
       "48   administration  0.000803   0.000331\n",
       "191         affairs  0.000334   0.000166\n",
       "166             aid  0.000368   0.000110\n",
       "161            also  0.000379   0.000349\n",
       "125          always  0.000457   0.000607\n",
       "51         american  0.000780   0.001398\n",
       "50            among  0.000791   0.000662\n",
       "194         another  0.000334   0.000460\n",
       "111       authority  0.000490   0.000184\n",
       "156          become  0.000390   0.000368\n",
       "39             best  0.000970   0.000607\n",
       "181          better  0.000345   0.000809\n",
       "155       blessings  0.000390   0.000092\n",
       "106        business  0.000502   0.000331\n",
       "131          called  0.000435   0.000423\n",
       "168           cause  0.000368   0.000460\n",
       "101       character  0.000502   0.000258\n",
       "139   circumstances  0.000423   0.000055\n",
       "138         citizen  0.000423   0.000294\n",
       "11         citizens  0.001906   0.001159\n",
       "97            civil  0.000524   0.000166\n",
       "79         commerce  0.000602   0.000147\n",
       "140          common  0.000423   0.000865\n",
       "99        condition  0.000513   0.000037\n",
       "63       confidence  0.000691   0.000441\n",
       "21         congress  0.001204   0.000386\n",
       "10     constitution  0.002118   0.000276\n",
       "104  constitutional  0.000502   0.000147\n",
       "146         control  0.000401   0.000221\n",
       "..              ...       ...        ...\n",
       "114           since  0.000479   0.000276\n",
       "43           spirit  0.000903   0.000975\n",
       "31            state  0.001059   0.000258\n",
       "2            states  0.003177   0.000773\n",
       "115           still  0.000479   0.000552\n",
       "74          subject  0.000635   0.000092\n",
       "59          support  0.000724   0.000441\n",
       "73           system  0.000647   0.000570\n",
       "130            take  0.000435   0.000441\n",
       "157       territory  0.000390   0.000055\n",
       "127       therefore  0.000446   0.000092\n",
       "30             time  0.001070   0.001968\n",
       "159           trade  0.000390   0.000258\n",
       "145            true  0.000412   0.000386\n",
       "103           trust  0.000502   0.000276\n",
       "13            union  0.001850   0.000405\n",
       "15           united  0.001594   0.000920\n",
       "3              upon  0.003032   0.001545\n",
       "19              war  0.001293   0.001012\n",
       "29             well  0.001115   0.000662\n",
       "172         whether  0.000357   0.000239\n",
       "58            whole  0.000724   0.000423\n",
       "87            whose  0.000557   0.000294\n",
       "182          wisdom  0.000345   0.000239\n",
       "60           within  0.000724   0.000515\n",
       "26          without  0.001148   0.000625\n",
       "66            world  0.000669   0.004838\n",
       "14            would  0.001616   0.001141\n",
       "44            years  0.000858   0.000993\n",
       "95              yet  0.000524   0.000589\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge the PreWWI and PostWWI dataframes (already sorted by probabilities in decending order) \n",
    "#in \"outer\" format which means the values show nan if NO tokens matches when merging the table\n",
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\", how=\"outer\")\n",
    "#Drop all the nan rows after merging tables\n",
    "pre_post_df.dropna(axis=0, how=\"any\", inplace= True)\n",
    "#Extract the top 200 Frequency rows of the table and sort the table by the column \"tokens\"\n",
    "pre_post_df = pre_post_df[0:200].sort_values(by =[\"tokens\"])\n",
    "#Drop the Frequency columns\n",
    "pre_post_df.drop(columns=[\"freq_x\", \"freq_y\"], inplace=True)\n",
    "pre_post_df\n",
    "#pre_post_df.to_csv(\"unigrams.txt\", index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_bigrams = list(nltk.bigrams(pre_words_lst2))\n",
    "fdbi = nltk.FreqDist(pre_bigrams)\n",
    "pre_df = pd.DataFrame(list(zip(fdbi.keys(), fdbi.values())), columns =[\"tokens\", \"freq\"])\n",
    "pre_df = pre_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "\n",
    "pre_df[\"pre_prob\"] = [round(freq / len(pre_words_lst2),7) for freq in pre_df.freq]\n",
    "\n",
    "\n",
    "pre_df = pre_df.sort_values(by = [\"pre_prob\"], ascending = False)\n",
    "#pre_df = pre_df.iloc[0:30000]\n",
    "#for i in range(0, pre_df.shape[0]):\n",
    " #   print(\"%15s %0.7f\"%(pre_df.iloc[i][\"tokens\"], pre_df.iloc[i][\"pre_prob\"]))\n",
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pre_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_bigrams = list(nltk.bigrams(post_words_lst2))\n",
    "fdbi = nltk.FreqDist(post_bigrams)\n",
    "post_df = pd.DataFrame(list(zip(fdbi.keys(), fdbi.values())), columns =[\"tokens\", \"freq\"])\n",
    "post_df = post_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "\n",
    "post_df[\"post_prob\"] = [round(freq / len(post_words_lst2),7) for freq in post_df.freq]\n",
    "\n",
    "\n",
    "post_df = post_df.sort_values(by = [\"post_prob\"], ascending = False)\n",
    "#post_df = post_df.iloc[0:30000]\n",
    "#for i in range(0, post_df.shape[0]):\n",
    " #   print(\"%15s %0.7f\"%(post_df.iloc[i][\"tokens\"], post_df.iloc[i][\"post_prob\"]))\n",
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_words_lst3 = []\n",
    "for word in pre_words_lst2:\n",
    "    if word in stopwords.words('english') or len(word)<=2:\n",
    "        pass\n",
    "    else:\n",
    "        pre_words_lst3.append(word)\n",
    "print(len(pre_words_lst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>pre_prob</th>\n",
       "      <th>post_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>(\", and)</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>(\", i)</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>(\", in)</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5439</th>\n",
       "      <td>(\", is)</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>(\", let)</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>(\", our)</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>(\", that)</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>(\", the)</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>(\", their)</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>(\", to)</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens  pre_prob  post_prob\n",
       "2067    (\", and)  0.000045   0.000074\n",
       "4325      (\", i)  0.000022   0.000018\n",
       "2274     (\", in)  0.000045   0.000037\n",
       "5439     (\", is)  0.000011   0.000018\n",
       "7771    (\", let)  0.000011   0.000018\n",
       "5679    (\", our)  0.000011   0.000018\n",
       "6005   (\", that)  0.000011   0.000018\n",
       "690     (\", the)  0.000123   0.000055\n",
       "2722  (\", their)  0.000033   0.000018\n",
       "694      (\", to)  0.000123   0.000018"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\")\n",
    "pre_post_df = pre_post_df.sort_values(by=[\"tokens\"])\n",
    "pre_post_df.drop(columns=[\"freq_x\", \"freq_y\"], inplace=True)\n",
    "pre_post_df[0:10]\n",
    "#pre_post_df.to_csv(\"unigrams.txt\", index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_df.set_index(\"tokens\").join(post_df.set_index(\"tokens\"),)\n",
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\")\n",
    "pre_post_df.columns = [\"unigram\", \"pre-WWI\",\"post-WWI\" ]\n",
    "pre_post_df = pre_post_df.sort_values(by = [\"pre-WWI\"], ascending=False)\n",
    "pre_post_df = pre_post_df[0:200]\n",
    "pre_post_df.sort_values(by = [\"unigram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating the preWWI list to down-casing all words\n",
    "for i in range(len(preWWI_lst)):\n",
    "    preWWI_lst[i] = inaugural.raw(preWWI_lst[i]).lower() #Store the preWWI data (ALL words) to the same list preWWI_lst\n",
    "\n",
    "#Iterating the postWWI_lst list to down-casing all words\n",
    "for i in range(len(postWWI_lst)):\n",
    "    postWWI_lst[i] = inaugural.raw(postWWI_lst[i]).lower() #Store the postWWI data (ALL words) to the same list postWWI_lst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part-1-3: For each dataset, compute the frequency distribution (i.e., probability) of unigrams and bigrams.  Then compute the following and write two output files -- one for unigrams and one for bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/KevQuant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c69ce2ae1974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwords_lst3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_lst2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "words_lst=[]\n",
    "for i in range(len(preWWI_lst)):\n",
    "    words_lst.append( word_tokenize(preWWI_lst[i]))\n",
    "\n",
    "words_lst2=[]\n",
    "for i in range(len(words_lst)):\n",
    "    words_lst2 +=words_lst[i]\n",
    "\n",
    "words_lst3=[]\n",
    "for word in words_lst2:\n",
    "    if (len(word)<=2) or (word in stopwords.words()):\n",
    "        pass\n",
    "    else:\n",
    "        words_lst3.append(word)\n",
    "\n",
    "fduni = nltk.FreqDist(words_lst3)\n",
    "len(fduni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preWWI_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6769"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict = {}\n",
    "for words in preWWI_lst:\n",
    "    words = word_tokenize(words)\n",
    "    for word in words:\n",
    "        if word in stopwords.words() or len(word)<=2:\n",
    "            pass\n",
    "        else:\n",
    "            if word.strip() in words_dict.keys():\n",
    "                words_dict[word.strip()]+=1\n",
    "            else:\n",
    "                words_dict[word.strip()]=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6761"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict2 = {}\n",
    "for k,v in words_dict.items():\n",
    "    if v >= 200:\n",
    "        pass\n",
    "    else:\n",
    "        words_dict2[k] = v\n",
    "words_dict2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(words_dict2.keys(), words_dict2.values())), columns =[\"tokens\", \"freq\"])\n",
    "df.sort_values(by=[\"tokens\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-01ea01abaaf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreWWI_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreWWI_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreWWI_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreWWI_words\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mpreWWI_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreWWI_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfreq_uni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "preWWI_words=[]\n",
    "for i in range(len(preWWI_lst)):\n",
    "    preWWI_words = preWWI_words +preWWI_lst[i]\n",
    "words = word_tokenize(preWWI_words)\n",
    "freq_uni = nltk.FreqDist(words)\n",
    "df = pd.DataFrame(list(zip(freq_uni.keys(), freq_uni.values())), columns = [\"tokens\", \"freq\"])\n",
    "#df.drop(labels=[5])\n",
    "df.sort_values(by = [\"tokens\"])\n",
    "#df.drop(labels=[5])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preWWI_words = []\n",
    "for i in range(len(preWWI_lst)):\n",
    "    preWWI_words = preWWI_words  + [preWWI_lst[i]]\n",
    "\n",
    "len(preWWI_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488573"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt =0\n",
    "for words in preWWI_lst:\n",
    "    cnt += len(words)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abced']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = [[\"abced\"], [\"fuahf fiaha\"]]\n",
    "\n",
    "aa= aa[0]+aa[1]\n",
    "word_tokenize(aa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6936"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst=[]\n",
    "for i in range(len(preWWI_lst)):\n",
    "    words_lst.append( word_tokenize(preWWI_lst[i]))\n",
    "\n",
    "words_lst2=[]\n",
    "for i in range(len(words_lst)):\n",
    "    words_lst2 +=words_lst[i]\n",
    "\n",
    "fduni = nltk.FreqDist(words_lst2)\n",
    "len(fduni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89345"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt=1\n",
    "for words in words_lst:\n",
    "    cnt += len(words)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams: 145735\n",
      "bigram 145734\n"
     ]
    }
   ],
   "source": [
    "unigrams = inaugural.words()\n",
    "print(\"unigrams:\", len(unigrams))\n",
    "bigrams = list(nltk.bigrams(unigrams))\n",
    "print(\"bigram\",len(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fduni = nltk.FreqDist(unigrams)\n",
    "print(\"length of unigrams Dict\",len(fduni))\n",
    "\n",
    "for (w,c) in fduni.items():\n",
    "    print(w,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Check out the frequency of each word link to another word next to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('-', 7), ('citizens', 16), ('Citizens', 1)])\n"
     ]
    }
   ],
   "source": [
    "cfd =  nltk.ConditionalFreqDist(bigrams)\n",
    "print(cfd[\"Fellow\"].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Conditional Probabilities of \n",
    "##(freq(\"-\") + freq(\"Fellow\")) divided by (Total freq(\"fellow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2916666666666667"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"Fellow\"].freq(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
