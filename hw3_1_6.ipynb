{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preWWI_lst = [] #Create list to collect preWWI (till 1913) fileids\n",
    "postWWI_lst =[] #Create list to collect postWWI (after 1917) fileids\n",
    "\n",
    "for fileid in inaugural.fileids():\n",
    "    if int(fileid[0:4])<=1913: #Collect the fileids before or on 1913\n",
    "        preWWI_lst.append(fileid)\n",
    "    elif int(fileid[0:4]) >1917: #Collect the fileids after on 1917\n",
    "        postWWI_lst.append(fileid)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89718\n"
     ]
    }
   ],
   "source": [
    "pre_words_lst = []\n",
    "for i in range(len(preWWI_lst)):\n",
    "    pre_words_lst.append(list(inaugural.words(preWWI_lst[i])))\n",
    "    \n",
    "pre_words_lst2 = []\n",
    "for i in range(len(pre_words_lst)):\n",
    "    pre_words_lst2 = pre_words_lst2 + pre_words_lst[i]    \n",
    "pre_words_lst2 = [word.lower() for word in pre_words_lst2]\n",
    "print(len(pre_words_lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54361\n"
     ]
    }
   ],
   "source": [
    "post_words_lst = []\n",
    "for i in range(len(postWWI_lst)):\n",
    "    post_words_lst.append(list(inaugural.words(postWWI_lst[i])))\n",
    "    \n",
    "post_words_lst2 = []\n",
    "for i in range(len(post_words_lst)):\n",
    "    post_words_lst2 = post_words_lst2 + post_words_lst[i]    \n",
    "post_words_lst2 = [word.lower() for word in post_words_lst2]\n",
    "print(len(post_words_lst2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cfdist(text, size=3):\n",
    "    \"\"\"\n",
    "    Extract unigrams and two-word tuples in the windows and create a \n",
    "    FrequencyDist dictionary for both (and returns them in a list)\n",
    "    \"\"\"\n",
    "    unigrams = []\n",
    "    tuples = []\n",
    "    # Scan over windows of the appropriate size.\n",
    "    for center in range(size, len(text)-size):\n",
    "        # enter the coocurrence (center word and each of all other words) in the dictionary\n",
    "        wunis = set()\n",
    "        wtuples = set() # for tuples in this context; set is to count only once\n",
    "        thisword = text[center]\n",
    "        \n",
    "        # iterate though the test of the window\n",
    "        for i in range(1, size+1): # i starts from 1 (center +/- i)\n",
    "            nextleft = text[center-i]\n",
    "            nextright = text[center+i]\n",
    "            # add them next word in this window's unigram set\n",
    "            wunis.add(nextleft)\n",
    "            wunis.add(nextright)\n",
    "            # create the next left tuple\n",
    "            if not thisword == nextleft:\n",
    "                if thisword < nextleft:\n",
    "                    tup = (thisword,nextleft)\n",
    "                else:\n",
    "                    tup = (nextleft,thisword)\n",
    "                # and add it in this window's tuple set\n",
    "                wtuples.add(tup) #\n",
    "            # create the next right tuple\n",
    "            if not thisword == nextright:\n",
    "                if thisword < nextright:\n",
    "                    tup = (thisword,nextright)\n",
    "                else:\n",
    "                    tup = (nextright,thisword)\n",
    "                # and add it in this window's tuple set\n",
    "                wtuples.add(tup) #\n",
    "        \n",
    "        # add all unigrams in the text tuples list\n",
    "        for wuni in wunis:\n",
    "            unigrams.append(wuni)\n",
    "        # add all tuples in the text tuples list\n",
    "        for wtup in wtuples:\n",
    "            tuples.append(wtup)\n",
    "            \n",
    "    # create a frequency dictionary from unigrams and tuples\n",
    "    ufd = nltk.FreqDist(unigrams)\n",
    "    cfd = nltk.FreqDist(tuples)\n",
    "    # and return the dictionaries in a list\n",
    "    return [ufd, cfd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Calling the make_cfdist function to find bigrams and unigrams for both pre-WWI and post-WWI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pre_words_lst2\n",
    "pre_ufd, pre_cfd = make_cfdist(text, size=3)\n",
    "\n",
    "text = post_words_lst2\n",
    "post_ufd, post_cfd = make_cfdist(text, size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##pre-WWI PMI Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_cfd_lst=[]\n",
    "for item in list(pre_cfd.keys()):\n",
    "    #print(item[0])\n",
    "    if len(item[0])>2 and len(item[1])>2 and item[0] not in stopwords.words() and item[1] not in stopwords.words():\n",
    "        pre_cfd_lst.append(item)\n",
    "        \n",
    "#Collect frequency column after filtering\n",
    "val_lst=[]\n",
    "for key in pre_cfd_lst:\n",
    "    val_lst.append(pre_cfd[key])\n",
    "\n",
    "#Collect bigrams after filtering    \n",
    "key1_lst = [item[0] for item in pre_cfd_lst]\n",
    "key2_lst = [item[1] for item in pre_cfd_lst]\n",
    "\n",
    "#Combine bigrams' terms and corresponding frequency\n",
    "df=pd.DataFrame(list(zip(key1_lst,key2_lst,val_lst)), columns=[\"key1\",\"key2\", \"freq\"])\n",
    "df.to_csv(\"pre_PMI_bigrams_filter.csv\",index=False) \n",
    "\n",
    "df=pd.read_csv(\"pre_PMI_bigrams_filter.csv\", index_col=False)\n",
    "\n",
    "#Calculate PMI for pre-WWI\n",
    "N = df.freq.sum()\n",
    "pre_PMI_lst=[]\n",
    "for i in range(0, df.shape[0]):\n",
    "    w1 = df.iloc[i].key1\n",
    "    w2 = df.iloc[i].key2\n",
    "    bigram_cnt = int(df.freq[df.key1==w1][df.key2==w2])\n",
    "    w1_cnt = pre_ufd[w1]\n",
    "    w2_cnt = pre_ufd[w2]\n",
    "\n",
    "    pre_PMI_lst.append(math.log2((bigram_cnt*(N-3)) / (w1_cnt*w2_cnt)))\n",
    "\n",
    "#Sort out all the bigrams include the word \"economy\" in either position for pre-WWI\n",
    "df[\"PMI\"] = pre_PMI_lst\n",
    "pre_arr = np.array(df)\n",
    "j=1\n",
    "pre_final_lst=[]\n",
    "for i in range(len(pre_arr)):\n",
    "    if \"economy\" in pre_arr[i]:\n",
    "        #print(j, pre_arr[i])\n",
    "        pre_final_lst.append(pre_arr[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##post-WWI PMI Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_cfd_lst=[]\n",
    "for item in list(post_cfd.keys()):\n",
    "    #print(item[0])\n",
    "    if len(item[0])>2 and len(item[1])>2 and item[0] not in stopwords.words() and item[1] not in stopwords.words():\n",
    "        post_cfd_lst.append(item)\n",
    "        \n",
    "#Collect frequency column after filtering\n",
    "val_lst=[]\n",
    "for key in post_cfd_lst:\n",
    "    val_lst.append(post_cfd[key])\n",
    "\n",
    "#Collect bigrams after filtering    \n",
    "key1_lst = [item[0] for item in post_cfd_lst]\n",
    "key2_lst = [item[1] for item in post_cfd_lst]\n",
    "\n",
    "#Combine bigrams' terms and corresponding frequency\n",
    "df=pd.DataFrame(list(zip(key1_lst,key2_lst,val_lst)), columns=[\"key1\",\"key2\", \"freq\"])\n",
    "df.to_csv(\"post_PMI_bigrams_filter.csv\",index=False) \n",
    "\n",
    "df=pd.read_csv(\"post_PMI_bigrams_filter.csv\", index_col=False)\n",
    "\n",
    "#Calculate PMI for post-WWI\n",
    "N = df.freq.sum()\n",
    "post_PMI_lst=[]\n",
    "for i in range(0, df.shape[0]):\n",
    "    w1 = df.iloc[i].key1\n",
    "    w2 = df.iloc[i].key2\n",
    "    bigram_cnt = int(df.freq[df.key1==w1][df.key2==w2])\n",
    "    w1_cnt = post_ufd[w1]\n",
    "    w2_cnt = post_ufd[w2]\n",
    "\n",
    "    post_PMI_lst.append(math.log2((bigram_cnt*(N-3)) / (w1_cnt*w2_cnt)))\n",
    "\n",
    "#Sort out all the bigrams include the word \"economy\" in either position for post-WWI\n",
    "df[\"PMI\"] = post_PMI_lst\n",
    "post_arr = np.array(df)\n",
    "j=1\n",
    "post_final_lst=[]\n",
    "for i in range(len(post_arr)):\n",
    "    if \"economy\" in post_arr[i]:\n",
    "        #print(j, post_arr[i])\n",
    "        post_final_lst.append(post_arr[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Print Bigrams and PMI on txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_final_lst = np.array(pre_final_lst)\n",
    "df_pre=pd.DataFrame(pre_final_lst, columns=[\"key1\",\"key2\",\"freq\",\"PMI\"])\n",
    "df_pre.sort_values(by=[\"PMI\"], ascending=False, inplace=True)\n",
    "df_pre=df_pre.head(45)\n",
    "df_pre.sort_values(by=[\"PMI\" ,\"key1\", \"key2\"], ascending=[False, True, True], inplace=True)\n",
    "\n",
    "post_final_lst = np.array(post_final_lst)\n",
    "df_post=pd.DataFrame(post_final_lst, columns=[\"key1\",\"key2\",\"freq\",\"PMI\"])\n",
    "df_post.sort_values(by=[\"PMI\"], ascending=False, inplace=True)\n",
    "df_post=df_post.head(45)\n",
    "df_post.sort_values(by=[\"PMI\" ,\"key1\", \"key2\"], ascending=[False, True, True], inplace=True)\n",
    "\n",
    "with open(\"pmi.txt\", \"w\") as f:\n",
    "    f.write(\"{0:>12}{1:>40}\\n\".format(\"pre-WWI\",\"post-WWI\" ))\n",
    "    f.write(\"%s\\n\"%(\"=\"*80))\n",
    "    for i in range(0, df_pre.shape[0]):\n",
    "        f.write(\"{0:2}  {1:<26} {2:0.7f}\".format(i+1,\"(\"+df_pre.iloc[i].key1+\", \"+df_pre.iloc[i].key2+\")\",df_pre.iloc[i].PMI,))\n",
    "        f.write(\"{0:>3}{1:<27} {2:0.7f}\\n\".format(\" \",\"(\"+df_post.iloc[i].key1+\", \"+df_post.iloc[i].key2+\")\",df_post.iloc[i].PMI,))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
