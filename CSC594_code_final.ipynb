{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1: Preliminary setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import gensim library\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-2: Load Dataset and Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.069399 min\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "df = pd.read_csv(\"./amazon_food_review/Reviews.csv\")\n",
    "df.columns\n",
    "end=time.time()\n",
    "print(\"Time taken %f min\"%(((end-start)/ 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 2.052629 min\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "sents=list(df.Text)\n",
    "sent_lst = []\n",
    "for line in sents:\n",
    "    #print(line)\n",
    "    for token_line in sent_tokenize(line):\n",
    "        #print(token_line)\n",
    "        sent_lst.append([token_line])\n",
    "end=time.time()\n",
    "print(\"Time taken %f min\"%(((end-start)/ 60)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences - Word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 8.140518 min\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "unigram_lst=[]\n",
    "for line in sent_lst:\n",
    "    #print(word_tokenize(line[0]))\n",
    "    unigram_lst.append(word_tokenize(line[0]))\n",
    "end=time.time()\n",
    "print(\"Time taken %f min\"%(((end-start)/ 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences - Word_tokenize (bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 2.212886 min\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "bigram_lst=[]\n",
    "for i in range(len(unigram_lst)):\n",
    "    #print(list(nltk.bigrams(word_lst[i])))\n",
    "    bigram_lst.append(list(nltk.bigrams(unigram_lst[i])))\n",
    "end=time.time()\n",
    "print(\"Time taken %f min\"%(((end-start)/ 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences - Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "sentence_stream = unigram_lst[0:100000]\n",
    "bigram_transformer = Phrases(sentence_stream,)\n",
    "#bigram = Phraser(phrases)\n",
    "#for line in sentence_stream:\n",
    " #   print(bigram[line])\n",
    "model_bi = Word2Vec(bigram_transformer[sentence_stream], size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram (Word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 3.560485 min\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "#Train the Word2Vec Model\n",
    "model_uni = Word2Vec(unigram_lst,size=100)  \n",
    "end=time.time()\n",
    "print(\"Time taken %f min\"%(((end-start)/ 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = model_uni.wv\n",
    "word_vectors.save(\"fname\")\n",
    "word_vectors = KeyedVectors.load(\"fname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the similar word (unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/KevQuant/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('quality', 0.6388070583343506),\n",
       " ('quality.', 0.6276007890701294),\n",
       " ('Appealing', 0.5973150730133057),\n",
       " ('quaility', 0.5848793983459473),\n",
       " ('res', 0.5840295553207397),\n",
       " ('Health', 0.5784130692481995),\n",
       " ('Prairie', 0.573959231376648),\n",
       " ('Patented', 0.5729488134384155),\n",
       " ('Oleic', 0.5575771331787109),\n",
       " ('Wetlands', 0.5526373982429504),\n",
       " ('communications', 0.5511699318885803),\n",
       " ('purity', 0.5504467487335205),\n",
       " ('Outstanding', 0.5502060651779175),\n",
       " ('Poisoning', 0.5429630279541016),\n",
       " ('Price', 0.5404151082038879),\n",
       " ('Luck', 0.5322612524032593),\n",
       " ('Nutritional', 0.5267941951751709),\n",
       " ('depiction', 0.5253832340240479),\n",
       " ('Actual', 0.523901104927063),\n",
       " ('affordability', 0.5232694149017334),\n",
       " ('healthiness', 0.5228654742240906),\n",
       " ('shellac', 0.521061897277832),\n",
       " ('placement', 0.5204619765281677),\n",
       " ('palatability', 0.51972496509552),\n",
       " ('Terrific', 0.5171420574188232),\n",
       " ('Activity', 0.5168048143386841),\n",
       " ('Excellant', 0.5162398815155029),\n",
       " ('Spa', 0.515681803226471),\n",
       " ('Effective', 0.5153339505195618),\n",
       " ('Aware', 0.5114071369171143),\n",
       " ('Import', 0.5101141929626465),\n",
       " ('Benefits', 0.5094850063323975),\n",
       " ('Pricewise', 0.5083799958229065),\n",
       " ('Kosugai', 0.508280336856842),\n",
       " ('thou', 0.5080642104148865),\n",
       " ('maltose', 0.5074993968009949),\n",
       " ('jerky/peppery', 0.5072811245918274),\n",
       " ('top-quality', 0.5067905187606812),\n",
       " ('service.', 0.5058326721191406),\n",
       " ('Habitant', 0.5056182146072388),\n",
       " ('Pricing', 0.5033726096153259),\n",
       " ('Inspection', 0.5028911828994751),\n",
       " ('THis', 0.5018501877784729),\n",
       " ('Karma', 0.501314640045166),\n",
       " ('Maturity', 0.500868022441864),\n",
       " ('Value', 0.5002269744873047),\n",
       " ('Rate', 0.49877744913101196),\n",
       " ('caliber', 0.4986428916454315),\n",
       " ('tis', 0.49856874346733093),\n",
       " ('B+', 0.49779266119003296),\n",
       " ('sugar/calorie', 0.49604129791259766),\n",
       " ('timeliness', 0.4957610070705414),\n",
       " (\"'Pure\", 0.49560287594795227),\n",
       " ('Longum', 0.4950020909309387),\n",
       " ('lunch/snack', 0.49495166540145874),\n",
       " ('Product', 0.494564950466156),\n",
       " ('grainfree', 0.49427443742752075),\n",
       " ('ORAC', 0.49389469623565674),\n",
       " ('Uhm', 0.49360817670822144),\n",
       " ('viability', 0.49336904287338257),\n",
       " ('Communication', 0.49298152327537537),\n",
       " ('thsi', 0.4926952123641968),\n",
       " ('Jaffrey', 0.4925687909126282),\n",
       " ('nonreturnable', 0.49244990944862366),\n",
       " ('Humbles', 0.4920753538608551),\n",
       " ('Feedback', 0.49048444628715515),\n",
       " ('Endurance', 0.4901459217071533),\n",
       " ('Appel', 0.4900650382041931),\n",
       " ('Service', 0.48985010385513306),\n",
       " ('Product.', 0.4897899031639099),\n",
       " ('Porcini', 0.4893456697463989),\n",
       " ('produxt', 0.48825111985206604),\n",
       " ('chairman', 0.4875103831291199),\n",
       " ('thie', 0.4868524968624115),\n",
       " ('Produce', 0.4861549735069275),\n",
       " ('Hepatitis', 0.4860658645629883),\n",
       " ('grade.', 0.4860605001449585),\n",
       " ('Excelent', 0.48569145798683167),\n",
       " ('shipcanned', 0.48557156324386597),\n",
       " ('pan-fired', 0.4853588938713074),\n",
       " ('Presentation', 0.4851265847682953),\n",
       " ('Texture', 0.4844099283218384),\n",
       " ('SnackMaster', 0.4842964708805084),\n",
       " ('Cosmetics', 0.4841880202293396),\n",
       " ('Cheap', 0.4830073118209839),\n",
       " ('Housekeeping', 0.4829356372356415),\n",
       " ('Al', 0.48260071873664856),\n",
       " ('Average', 0.482306569814682),\n",
       " ('0/5', 0.4821757972240448),\n",
       " ('concentrations', 0.48083001375198364),\n",
       " ('quailty', 0.47981858253479004),\n",
       " ('raters', 0.47939497232437134),\n",
       " ('nutritions', 0.47886523604393005),\n",
       " ('EDEN', 0.4787229895591736),\n",
       " ('Edible', 0.477924644947052),\n",
       " ('Testing', 0.477647602558136),\n",
       " ('Tasting', 0.47738757729530334),\n",
       " ('4Health', 0.47731271386146545),\n",
       " ('cast.', 0.4773080050945282),\n",
       " ('Fine', 0.47703269124031067)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(model.wv['product'])\n",
    "model_uni = word_vectors\n",
    "print(len(list(model_uni.wv.vocab)))\n",
    "model_uni.wv.most_similar([\"Quality\"], topn=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the similar word (bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dog_food', 0.7898504137992859),\n",
       " ('formula', 0.6773414611816406),\n",
       " ('treat', 0.6714749336242676),\n",
       " ('treats', 0.6644978523254395),\n",
       " ('foods', 0.6483891010284424),\n",
       " ('dog', 0.630307137966156),\n",
       " ('him', 0.6175776124000549),\n",
       " ('cat', 0.6128666996955872),\n",
       " ('stuff', 0.6050869226455688),\n",
       " ('cat_food', 0.5984747409820557),\n",
       " ('her', 0.5888310074806213),\n",
       " ('baby_food', 0.5838652849197388),\n",
       " ('positive_feedback', 0.5764857530593872),\n",
       " ('weight', 0.5717586278915405),\n",
       " ('us', 0.5709643363952637),\n",
       " ('teeth', 0.5659422874450684),\n",
       " ('product', 0.558136522769928),\n",
       " ('plant', 0.5493203401565552),\n",
       " ('baby', 0.5460817813873291),\n",
       " ('meds', 0.5451784133911133),\n",
       " ('canned_food', 0.5450034141540527),\n",
       " ('my_son', 0.5406973361968994),\n",
       " ('diet', 0.5371577739715576),\n",
       " ('something', 0.5345394015312195),\n",
       " ('medicine', 0.5301904678344727),\n",
       " ('popcorn', 0.5300434827804565),\n",
       " ('way', 0.5281774997711182),\n",
       " ('his', 0.5228219032287598),\n",
       " ('products', 0.5182557106018066),\n",
       " ('she', 0.5164166688919067),\n",
       " ('them', 0.5155109763145447),\n",
       " ('company', 0.5140153169631958),\n",
       " ('Canidae', 0.5088779330253601),\n",
       " ('he', 0.5075368881225586),\n",
       " ('allergies', 0.5014228820800781),\n",
       " ('recipe', 0.49457263946533203),\n",
       " ('family', 0.48943203687667847),\n",
       " ('my_daughter', 0.4884780943393707),\n",
       " ('vet', 0.48362496495246887),\n",
       " ('puppy', 0.48230642080307007),\n",
       " ('sick', 0.47998571395874023),\n",
       " ('everyone', 0.47920048236846924),\n",
       " ('doing', 0.47315508127212524),\n",
       " ('incentive', 0.47117453813552856),\n",
       " ('item', 0.4711568355560303),\n",
       " ('healthy', 0.4705985188484192),\n",
       " ('these_treats', 0.4696377217769623),\n",
       " ('pill', 0.46728894114494324),\n",
       " ('grass', 0.467277467250824),\n",
       " ('older', 0.46652573347091675),\n",
       " ('joints', 0.46481066942214966),\n",
       " ('life', 0.4631847143173218),\n",
       " ('dogs', 0.4631779193878174),\n",
       " ('problems', 0.4627082943916321),\n",
       " ('gum', 0.46021056175231934),\n",
       " ('guests', 0.45955735445022583),\n",
       " ('health', 0.4573698043823242),\n",
       " ('eating', 0.456748366355896),\n",
       " ('things', 0.45651280879974365),\n",
       " ('dry_food', 0.4564412534236908),\n",
       " ('pills', 0.45556584000587463),\n",
       " ('toll', 0.45554623007774353),\n",
       " ('skin', 0.45393913984298706),\n",
       " ('snacks', 0.4533678889274597),\n",
       " ('recipes', 0.4526432156562805),\n",
       " ('gas', 0.4525705575942993),\n",
       " ('option', 0.45197755098342896),\n",
       " ('pets', 0.4512876272201538),\n",
       " ('plants', 0.4501529633998871),\n",
       " ('town', 0.4484255909919739),\n",
       " ('animals', 0.4466843008995056),\n",
       " ('house', 0.44612622261047363),\n",
       " ('head', 0.445628821849823),\n",
       " ('everything', 0.4453195035457611),\n",
       " ('own', 0.4453144371509552),\n",
       " ('children', 0.4450981020927429),\n",
       " ('who', 0.4446660876274109),\n",
       " ('cereal', 0.44453656673431396),\n",
       " ('bored', 0.44066858291625977),\n",
       " ('brand', 0.44055747985839844),\n",
       " ('quest', 0.43836694955825806),\n",
       " ('stock', 0.43723317980766296),\n",
       " ('reactions', 0.43605315685272217),\n",
       " ('formulation', 0.4357972741127014),\n",
       " ('how', 0.4354300796985626),\n",
       " ('babies', 0.4351550340652466),\n",
       " ('getting', 0.4351177513599396),\n",
       " ('home', 0.43461841344833374),\n",
       " ('She', 0.4345508813858032),\n",
       " ('heart', 0.4322071671485901),\n",
       " ('tray', 0.4318343997001648),\n",
       " ('kids', 0.43122798204421997),\n",
       " ('helping', 0.4306080937385559),\n",
       " ('shelf', 0.4299100339412689),\n",
       " ('going', 0.42951834201812744),\n",
       " ('kitchen', 0.4275447130203247),\n",
       " ('breath', 0.4275311231613159),\n",
       " ('snack', 0.426987886428833),\n",
       " ('fur', 0.4244990348815918),\n",
       " ('my_husband', 0.4243446886539459)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_bi.most_similar([\"good\",\"food\"], topn=100)\n",
    "print(len(list(model_bi.wv.vocab)))\n",
    "#list(model_bi.wv.vocab)\n",
    "model_bi.wv.most_similar([\"food\"], topn=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-3: Obtaining Feature Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": false
   },
   "outputs": [],
   "source": [
    "#Embeding for \"service\" word\n",
    "#print(model.wv['Rooms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848\n",
      "{'World', 'density', '_', 'Gail', 'container', 'oils', 'WF', 'Valley', 'concoction', 'Milling', 'Elbows', 'quaility', 'ingredient', 'Pinch', 'materials', 'Spices', 'deal-', 'price..', 'CVS', 'investment', 'Mayonnaise', 'news', 'hopes.', 'Taste', 'vase', 'Public', 'poisons', 'flavor', 'Supplies', 'receipes', '101812', 'Formula', 'descriptions', 'Organix', 'product.', 'contributors', 'Foods.', 'fizzy', 'Activity', 'Health', 'differences', 'coating', 'melts', 'values', 'sprouter', 'caliber', 'Hairball', 'Supplier', 'Pretzels', 'Sounded', 'coupon', 'All-Purpose', 'Yam', 'food..', 'Patented', 'Storage', 'posts', 'Basket', 'DHA', 'pro/con', 'Target', 'Solubles.', 'catfood', 'babyfood', 'Litter', 'results.', 'aspects', 'Value', 'aftertaste', 'labeling', 'DVD', 'Outlet', 'Indoor', 'humidity', \"'s\", 'ratios', 'petrolatum', 'PetSmart', 'Beneful', 'kibble', 'Jewel', 'colours', 'reveiws', 'Shawn', 'Institute', 'isomalt', 'Dravis', 'mouthfeel', 'temperaments', 'statements', 'ingredent', 'tang', '357', 'grade', 'tin', 'shipping', 'no-no', 'Bastianich', '1997.', 'execution', 'crunch', 'product..', 'Texture', 'meals', 'resource', 'Tyson', 'yeasts', 'lid', 'Smells', 'Rustichella', 'healthiness', 'crunchiness', 'wrapper', 'varieties', 'Excellant', 'sharpness', 'Veterinary', 'taste.', 'Rebecca', 'Automatic', 'listing', 'Rate', 'Oleic', 'outcome', 'Idea', 'Quick', 'Tastes', 'deal', 'shipping/packaging', 'top-quality', 'Merchant', 'chow', 'Iams', 'oolongs', 'content', 'feedback', 'Notes', 'Wal', 'Bamboo', 'Sincerely', 'Deception', 'after-taste', 'Porkhide', 'concentration', 'motabolism', 'supplements', 'adjective', 'been-there-done-that', 'Worthington', 'pricing', 'snacks', 'aroma', 'graphics', 'Chow', 'pH', 'Lover', 'dissolves', 'BOTTOM', 'trimesters', 'installment', 'quantity', '1-star', 'package', 'Orijen', 'fructose', 'poster', 'Actual', '1.4oz', 'Evo', 'Ronzoni', 'Fields', 'Gift', 'Waves', 'Pros', 'ad', 'Eats', 'Burger', 'catnip', 'Biscotti', 'quality', 'Notable', 'Show', 'jerky/peppery', 'bonus', 'luck', 'Packing', 'lids', 'Everything', 'Guide', 'Karmal', 'websites', 'tast', 'Vendor', 'attraction', 'eclipsed', 'Ricemilk', 'foods', 'Walmart', 'Family', 'shipping/handling', 'Disclosure', 'Downtown', 'products', 'affordability', 'Frustration-Free', 'Petsmart', 'jerkies', 'Cakes', 'brands', 'Karma', 'Celebration', 'GUARANTEED', 'Wrestling', 'link', '**', 'tastes', 'sellers', 'Hearts', 'bitterness', 'ingrdients', 'packaging.', 'well-packaged', 'innovation', 'wrappings', 'credos', 'smells', '115mg', 'cup-a-joe', 'Rescue', 'monday', 'smells/tastes', 'Sturdy', 'Description', 'condition.', 'things', 'fruits', 'chews/treats', 'catfoods', \"5'-Monophosphate\", 'Pharmacy', 'frutose', 'Tasted', 'liner', 'Shipment', 'depiction', 'Alternative', 'Material', 'discussion', 'detail', 'EVO', 'cereal', 'Product.', 'discription', 'thing..', 'Approved', 'additives', 'Cultures', 'Council', 'vis-versa', 'Times', 'tasting', 'image', 'Expo', 'produxt', 'Spa', 'Item', 'Botanicals', 'entries', 'Food.', 'smelling', 'Arrived', 'Product', 'ps', 'Education', 'tis', 'honeys', 'letter', 'disclaimer', 'Mixers', '********', 'caviar', 'arrangement', 'food.', 'pic', 'tunas', 'tase', 'cuisines', 'Soups', 'palatability', 'Goods', 'option', 'Michaels', 'agents', 'eye-opener', 'sounds', 'Snacks', 'conditioner', 'Mills', 'service.', 'B000Z9DMX8', 'GMOs', 'promotion', 'communication', 'Tasting', 'Inspection', 'goodies', 'treat', 'shape', 'sources', 'foods.', 'Gene', 'shipper', 'Grocer', 'Feels', 'countries', 'Maturity', 'Info', 'ANALYSIS', 'sweetens', 'toy', 'Pedigree', 'label', 'offerings', 'makes', 'Baklava', 'vendor', 'deal.', 'Its', 'criterion', 'sourness', 'ingredients-', 'positives', 'Sounds', 'alternatives', 'idea', 'meats', 'standards', 'Deluca', 'ratings', 'taste-', 'FACTS', 'Supermarkets', 'taste', 'preparations', 'bundle', 'Prairie', 'Select', 'dogfoods', 'Products', 'is', 'Stuff', 'grains', 'Reviewed', 'invention', 'Effective', 'Items', 'Guaranteed', 'Pricing', 'qualities', 'Co', 'formula', 'seals', 'smell', 'Oct', 'saffron', 'Tongue', 'Fillin', 'Kosugai', 'commenter', 'zing', 'Cupcakes', 'Night', 'design', 'Fussie', 'tartness', 'innova', 'book', 'lable', 'fragrance', 'wise-old-grain', 'sitter', 'sweetness', 'Grits', 'IKEA', 'versions', 'vaule', 'Mart', 'Housekeeping', 'placement', 'probability', 'Journal', 'Costco', 'cathartic', 'Sprouts', 'Sams', 'Positives', 'elements', 'post', 'Kitten', 'dressing', 'ingridient', 'shape.', 'Presentation', 'ingredient.', 'Store', 'smelled', 'CLEARLY', 'oleic', 'Canidae', 'Benefits', 'Home', 'Sonoma', 'Rounds', 'glycemic', 'page', 'undertaste', 'Safeway', 'Crackers', 'Features', 'items', 'reviewers', 'statement', 'products.', 'looks', 'powders', 'tuna', 'vendors', 'scent', 'taste.This', 'Wells', 'kit', 'Tooth', 'Cookbook', 'flavor-', 'Sale', 'Paycheck', 'Summary', 'octane', 'reply', 'twang', 'review..', 'Mature', 'material', 'Connection', 'price.', 'Hummus', 'Eukanuba', 'Market', 'suppliers', 'broths', 'Ann', 'OVERALL', 'milo', 'kick', 'bars', 'percentage', 'attributes', 'Pyrenees', 'treats', 'remarks', '7/10', 'cuncern', 'brand', 'adhesive', 'objection', 'Poppy', 'CONS', 'flavours', 'thing', 'Co.', 'soup', 'producers', 'Northern', 'food-', 'vender', 'Aug', 'bargin', 'value', 'priced', 'entrees', 'Supply', 'nigrum', 'Homemade', 'pamphlet', 'ASIN', 'site', 'blog', 'email', 'ZP', 'litters', 'Cherrybrook', 'caffeination', 'smoothness', 'Terrific', 'Meijer', 'flavor.', 'Classics', 'Friday', 'laboratories', 'photographs', 'quality.', 'Seller', 'Safely', 'Kovacich', 'Wellness', 'Active', 'guality', 'hexane', 'situation', 'Halva', 'feels', 'Analysis', 'Seattles', 'Shipping', 'Buns', 'Result', 'Recommended.', 'foamy', '***', 'taste/smell', 'Pricewise', 'Adult', 'Import', 'facts', 'Gunner', 'Pan', 'prunes', 'EVALUATION', 'Plant', 'deals', 'price-', 'Psyllium', 'Polenta', 'Professor', 'high-quality', 'tasted', 'Delivery', 'buyer', 'PH', 'product-', 'friskies', 'quality/cost', 'Uhm', 'Kroger', 'drawbacks', 'comment', 'hydrates', 'creaminess', 'print', 'Drug', 'Preparation', 'smelt', 'maltose', 'ingedients', 'picture', 'Poisoning', 'weave', 'Replacement', 'Grips', 'works', 'Smell', 'behaviors', 'Steal', 'Michelle', 'protein/fiber', 'texture', 'Homegrown', 'Shops', 'Smelled', 'Recipes', 'Dog', 'flavors', 'procuct', 'response', 'Habitant', 'Ingredient', 'Goody', 'viscosity', 'video', 'freshness', 'expereience', 'shellac', 'Shipped', 'Toxicology', 'Inc', 'Innova', 'thou', 'supplier', 'proteins', 'flours', 'Tuscan', 'Updated', 'Games', 'Price', 'ingredients.', 'Corp.', 'Foil', 'Depot', 'selection', 'dishes', 'Soul', 'photo', 'KV', 'Wetlands', 'Brands', 'beverage', 'graders', 'comments', 'shampoo', 'syrups', 'album', 'specials', 'Service', 'PROS', 'Outstanding', 'CRUELTY-FREE', '~The', 'Dieters', 'Trading', 'meat', 'substances', 'flavoring', 'Plants', 'nip', 'options', 'treats/food', 'odor', 'review.', 'Packaging', 'Communication', 'Luck', 'Legacy', 'JuicePlus', 'feature', 'method', 'pricing.', 'success', 'sale', 'word', 'bargain', 'advertising', 'tones', 'Mays', 'mercury', 'jerky', 'Meaty', 'derivative', 'seal', 'possibilities', 'Pies', 'discount', 'Bees', 'ph', 'seller', 'components', 'bonsai', 'shampoos', 'price', 'forum', 'Broadway', 'sentence', 'brochure', 'All-In-One', 'Publix', 'Sense', 'Subtilis', 'ingredient-', 'pyrenees', 'tastes/smells', 'Wholesale', 'additions', 'variants', 'bubbly', 'Sixth', 'packing', 'Job', 'rubs', 'elevations', 'Kyle', '8th', 'PROBAR', 'Looks', 'advantages', 'savings', 'Package', 'Author', 'paragraph', 'fruitcake', 'reviews', 'diets', 'description', 'Appealing', 'gum', 'households', 'presentation', 'ingriedient', 'pastas', 'essence', 'symbol', 'flavour', 'Nov', 'Oats', 'objects', 'food', 'reviewer', 'B+', 'plant', 'product', 'Company', 'mustards', 'Amortization', 'res', 'ghee', 'concept', 'forms', 'Vegetarian', 'smell/taste', 'Officials', 'turn-offs', 'constituent', 'item', 'Inc.', 'Aware', 'Supermarket', 'advertisement', 'Quality', 'P.S', 'Center', 'reusable.', 'Mine', 'ingredience', 'autolyzed', 'consistency', 'Conclusion', 'indicator', 'chicken-by-products', 'PASSION', 'Dec.', 'sounded', 'milks', '//www.amazon.com/gp/product/B0051BRR4A', 'wrapping', 'reveiw', 'litter', 'Basin', 'ingrediants', 'praise', 'sauce', 'packaging', 'Grocery', 'Tiki', 'dogfood', 'stats', 'Rippin', 'companies', 'tasts', 'Global', 'gripes', 'J.', 'Space', 'booklet', 'rating', 'Fuhgeddaboudit', 'Longum', 'Dec', 'premium', 'copy', 'Dean', 'reputation', 'Complete', 'overtone', 'Goodness', 'richness', 'design.', 'Administration', 'TOTW', 'integrity', 'bargains', 'Alpha', 'device', 'quailty', 'probiotics', 'environmentally', 'allergens', 'Dynamic', 'Guard', 'Mad', 'pitfall', '\\\\', 'EVOO', 'smokiness', 'communications', 'Superfoods', 'concentrations', '26th', 'evaluation', 'compounds', 'headline', 'KD', 'sign.', 'calories/sugar', 'Depression', 'Food', 'Pet', 'color', 'lemonade', 'product.I', 'Dane', 'M.', 'NUTRITION', 'observation', 'formulas', 'Nutritional', 'jerkys', 'altitude', 'UPDATED', 'descriptor', 'merchant', 'Honeyville', 'varities', 'cereals', 'Deal', 'Dollar', 'purity', 'THis', 'granolas', 'Cora', 'spiciness', 'tea', 'message', 'Bil-Jac', 'ingredients', 'mini-chip', 'owners', 'oatmeals', 'Pets', 'undertone', 'pluses', 'company', 'prices', 'others.yummy', 'foil', 'manufacturer', 'kibbles', 'Mins', 'Fund', 'stuff', 'Decay'}\n"
     ]
    }
   ],
   "source": [
    "#Similar word vector as the word \"service\".\n",
    "#HW5: 'room' (top 50), 'rooms' (top 50), 'Room' (top 20) and 'Rooms' (top 20)\n",
    "\n",
    "#Initialize the list of feature\n",
    "feature_lst= []\n",
    "for word in [\"food\",\"Food\", \"foods\", \"Foods\", \"quality\",\"Quality\", \"review\", \"Review\", \n",
    "             \"ingrediants\",\"ingrediant\", \"taste\", \"tastes\", \"product\", \"Product\",\"products\",\n",
    "            \"Products\", \"Packaging\", \"packaging\", \"deal\", \"Deal\"]:\n",
    "    #if word[0]==\"r\": #Find top 50 \n",
    "    word_lst = list(model_uni.most_similar(word, topn=50))\n",
    "    for i in range(len(word_lst)):\n",
    "        #print(word_lst[i][0])\n",
    "        feature_lst.append(word_lst[i][0])\n",
    "    #else:\n",
    "     #   word_lst = list(model_uni.most_similar(word, topn=50))\n",
    "      #  for i in range(len(word_lst)):\n",
    "            #print(word_lst[i][0])\n",
    "       #     feature_lst.append(word_lst[i][0])\n",
    "print(len(list(set(feature_lst))))\n",
    "print(set(feature_lst))            \n",
    "feature_lst = list(set(feature_lst))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-4: hand-write some patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Grammar: NP\n",
    "#sent = \"Nicely painted rooms\"\n",
    "#sent = \"clean room\"\n",
    "#sent = \"very large painted bathroom window\"\n",
    "#sent = \"very large painted bathroom\"\n",
    "#sent= \"modern shower head\"\n",
    "\n",
    "############ clause1a\n",
    "sent = \"room was spotless\"\n",
    "sent = \"the Beds were very comfortable\"\n",
    "sent = \"The walls are thin\"\n",
    "sent = \"The rooms seemed pretty clean\"\n",
    "sent = \"the beds were excellent\"\n",
    "sent = \"the bed is very nice\"\n",
    "sent = \"the bottles are not too large\"\n",
    "sent = \"the bottles are too large\"\n",
    "sent =\"the bed are not good\"\n",
    "sent =\"the bed are not very good enough\"\n",
    "sent =\"the bed are not very good\"\n",
    "\n",
    "############ clause1b\n",
    "sent = \" room was spotless and clean\"\n",
    "sent = \" room was spotless and bed is soft\"\n",
    "sent = \" the room is pretty beautiful and the bed is pretty soft\"\n",
    "sent = \" the room is  beautiful and the bed is very soft\"\n",
    "\n",
    "############ clause2\n",
    "sent = \" John loves the room\"\n",
    "sent = \" She likes this beautiful room\"\n",
    "sent = \"I love the room\"\n",
    "\n",
    "############ clause2b\n",
    "#sent = \"She likes this beautiful room and soft bed\"\n",
    "#sent = \"She likes this room and the bed\"\n",
    "\n",
    "\n",
    "############clause3\n",
    "#sent = \"the bottles are way too large\"\n",
    "\n",
    "############clause4\n",
    "#sent=\"the sheet did not smell good\"\n",
    "\n",
    "############clause5\n",
    "sent = \"I love room with many windows\"\n",
    "\n",
    "############Extra\n",
    "sent = \"I love a car which is red made in europe\"\n",
    "sent=  \" the rooms are comfortable\"\n",
    "\n",
    "sent= \". Is there an American airlines flight from Philadelphia to Dallas? \"\n",
    "\n",
    "\n",
    "print(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "sent =  nltk.word_tokenize(sent)\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "review_lst=[]\n",
    "tree = cp.parse(nltk.pos_tag(sent))\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() ==\"NP\":\n",
    "        print(\"NP\")\n",
    "        print(subtree.leaves())    \n",
    "    if subtree.label() ==\"clause1a\":\n",
    "        print(\"clause1a\")\n",
    "        print(subtree.leaves())\n",
    "        review_lst.append(subtree.leaves())\n",
    "    if subtree.label() ==\"clause1b\":\n",
    "        print(\"clause1b\")\n",
    "        print(subtree.leaves())\n",
    "        review_lst.append(subtree.leaves())\n",
    "    if subtree.label() ==\"clause2a\":\n",
    "        print(\"clause2a\")\n",
    "        print(subtree.leaves())\n",
    "        review_lst.append(subtree.leaves())\n",
    "    if subtree.label() ==\"clause2b\":\n",
    "        print(\"clause2b\")\n",
    "        print(subtree.leaves())\n",
    "        review_lst.append(subtree.leaves())\n",
    "    if subtree.label() ==\"clause3\":\n",
    "        print(\"clause3\")\n",
    "        print(subtree.leaves())\n",
    "        review_lst.append(subtree.leaves())\n",
    "    if subtree.label() ==\"clause4\":\n",
    "        print(\"clause4\")\n",
    "        print(subtree.leaves())\n",
    "        review_lst.append(subtree.leaves())\n",
    "    if subtree.label() ==\"clause5\":\n",
    "        print(\"clause5\")\n",
    "        print(subtree.leaves())\n",
    "        review_lst.append(subtree.leaves())\n",
    "\n",
    "\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex grammar\n",
    "grammar = r\"\"\"\n",
    "JN:{<RB.*|JJ.*>}\n",
    "\n",
    "NP:{<DT|PRP\\$>?<RB>*<JN|VB.*>+<NN.*>+} \n",
    "\n",
    "clause1a:{<DT|PRP>?<NN.*>+<VB.*>+<JN>+<JJ.*>*}\n",
    "        \n",
    "clause1b:{<clause1a>+<,|CC>+<JN|NN.*>+<JN|NN.*>*}      \n",
    "        {<clause1a>+<,|CC>+<clause1a>+}\n",
    "\n",
    "clause2a:{<NN.*|PRP.*>+<IN|VB.*>+<DT>*<NP|NN.*>+}\n",
    "\n",
    "clause2b:{<clause2a>+<,|CC>+<DT>*<JN|NN.*|NP>+}      \n",
    "        {<clause2a>+<,|CC>+<clause2a>+}\n",
    "        \n",
    "clause3: {<DT>?<NN.*>+<NP>+<JN>*}\n",
    "\n",
    "clause4: {<clause1a>+<VB.*>+<JN>*}\n",
    "\n",
    "clause5: {<NN.*|PRP.*><NP>+<IN>+<NP>*}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-5: Extract and Parse all reviews in the city \"Chicago\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The mouth says, \"How do I love thee, let me count the ways...\"<br />If you like apple products a must have item.  The only draw back, shipping cost.  These are very heavy.',\n",
       " \"Arrived slightly thawed. My parents wouldn't accept it. However, the company was very helpful and issued a full refund.\",\n",
       " \"The crust on these tarts are perfect.  My husband loves these, but I'm not so crazy about them.  They are just too sour/tart for my taste.  I'll eat the crust and hubby takes my filling.  My kids think they're great, so maybe it's just me.\"]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[100:103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', '.'], ['The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', '.'], ['My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', 'most', '.']]\n",
      "Time taken 0.000821 min\n",
      "# of segments 7\n",
      "df_lst1: [0.5625]\n",
      "df_lst1: [0.5625, 0.0]\n",
      "df_lst1: [0.5625, 0.0, 0.0]\n",
      "df_lst1: [0.5625, 0.0, 0.0, 0.041666666666666664]\n",
      "df_lst1: [0.5625, 0.0, 0.0, 0.041666666666666664, 0.0]\n",
      "df_lst1: [0.5625, 0.0, 0.0, 0.041666666666666664, 0.0, 0.0]\n",
      "df_lst1: [0.5625, 0.0, 0.0, 0.041666666666666664, 0.0, 0.0, 0.0]\n",
      "[['Product', 'arrived', 'labeled', 'as', 'Jumbo', 'Salted', 'Peanuts', '...', 'the', 'peanuts', 'were', 'actually', 'small', 'sized', 'unsalted', '.'], ['Not', 'sure', 'if', 'this', 'was', 'an', 'error', 'or', 'if', 'the', 'vendor', 'intended', 'to', 'represent', 'the', 'product', 'as', '``', 'Jumbo', \"''\", '.']]\n",
      "Time taken 0.000991 min\n",
      "# of segments 1\n",
      "df_lst1: [0.0]\n",
      "[['This', 'is', 'a', 'confection', 'that', 'has', 'been', 'around', 'a', 'few', 'centuries', '.'], ['It', 'is', 'a', 'light', ',', 'pillowy', 'citrus', 'gelatin', 'with', 'nuts', '-', 'in', 'this', 'case', 'Filberts', '.'], ['And', 'it', 'is', 'cut', 'into', 'tiny', 'squares', 'and', 'then', 'liberally', 'coated', 'with', 'powdered', 'sugar', '.'], ['And', 'it', 'is', 'a', 'tiny', 'mouthful', 'of', 'heaven', '.'], ['Not', 'too', 'chewy', ',', 'and', 'very', 'flavorful', '.'], ['I', 'highly', 'recommend', 'this', 'yummy', 'treat', '.'], ['If', 'you', 'are', 'familiar', 'with', 'the', 'story', 'of', 'C.S', '.'], ['Lewis', \"'\", '``', 'The', 'Lion', ',', 'The', 'Witch', ',', 'and', 'The', 'Wardrobe', \"''\", '-', 'this', 'is', 'the', 'treat', 'that', 'seduces', 'Edmund', 'into', 'selling', 'out', 'his', 'Brother', 'and', 'Sisters', 'to', 'the', 'Witch', '.']]\n",
      "Time taken 0.001269 min\n",
      "# of segments 4\n",
      "df_lst1: [0.09375]\n",
      "df_lst1: [0.09375, 0.041666666666666664]\n",
      "df_lst1: [0.09375, 0.041666666666666664, 0.0]\n",
      "df_lst1: [0.09375, 0.041666666666666664, 0.0, 0.25]\n",
      "[['If', 'you', 'are', 'looking', 'for', 'the', 'secret', 'ingredient', 'in', 'Robitussin', 'I', 'believe', 'I', 'have', 'found', 'it', '.'], ['I', 'got', 'this', 'in', 'addition', 'to', 'the', 'Root', 'Beer', 'Extract', 'I', 'ordered', '(', 'which', 'was', 'good', ')', 'and', 'made', 'some', 'cherry', 'soda', '.'], ['The', 'flavor', 'is', 'very', 'medicinal', '.']]\n",
      "Time taken 0.001407 min\n",
      "# of segments 4\n",
      "df_lst1: [-0.08333333333333333]\n",
      "df_lst1: [-0.08333333333333333, -0.125]\n",
      "df_lst1: [-0.08333333333333333, -0.125, 0.0]\n",
      "df_lst1: [-0.08333333333333333, -0.125, 0.0, 0.0]\n",
      "[['Great', 'taffy', 'at', 'a', 'great', 'price', '.'], ['There', 'was', 'a', 'wide', 'assortment', 'of', 'yummy', 'taffy', '.'], ['Delivery', 'was', 'very', 'quick', '.'], ['If', 'your', 'a', 'taffy', 'lover', ',', 'this', 'is', 'a', 'deal', '.']]\n",
      "Time taken 0.001537 min\n",
      "# of segments 3\n",
      "df_lst1: [0.0]\n",
      "df_lst1: [0.0, 0.0]\n",
      "df_lst1: [0.0, 0.0, 0.0]\n",
      "avg [0.08630952380952381, 0.0, 0.09635416666666666, -0.05208333333333333, 0.0]\n",
      "label [1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "negcontractions = [\"n't\", \"'t\"]\n",
    "negwords = ['not', 'no', \"nor\"]\n",
    "label_lst = []\n",
    "avg_label_score_lst = []\n",
    "\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "sents=list(df.Text)\n",
    "token_line_lst = []\n",
    "\n",
    "\n",
    "for line in sents[0:5]:\n",
    "    token_line_lst=[]\n",
    "    #print(line)\n",
    "    for token_line in sent_tokenize(line):\n",
    "        #print(token_line)\n",
    "        #print(token_line)\n",
    "        token_line_lst.append(token_line)\n",
    "    \n",
    "    sentlist = [word_tokenize(line) for line in token_line_lst]\n",
    "    print(sentlist)\n",
    "    \n",
    "    review_lst=[]    \n",
    "    for sent in sentlist: #Iterate the Review sentlist \n",
    "        tree = cp.parse(nltk.pos_tag(sent)) #pos_tag the sentence and parse the sentence\n",
    "        #print(tree)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() ==\"NP\":\n",
    "                #print(\"NP\")\n",
    "                #print(subtree.leaves())    \n",
    "                review_lst.append(subtree.leaves())\n",
    "            if subtree.label() ==\"clause1a\":\n",
    "                #print(\"clause1a\")\n",
    "                #print(subtree.leaves())\n",
    "                review_lst.append(subtree.leaves())\n",
    "            if subtree.label() ==\"clause1b\":\n",
    "                #print(\"clause1b\")\n",
    "                #print(subtree.leaves())\n",
    "                review_lst.append(subtree.leaves())\n",
    "            if subtree.label() ==\"clause2a\":\n",
    "                #print(\"clause2a\")\n",
    "                #print(subtree.leaves())\n",
    "                review_lst.append(subtree.leaves())\n",
    "            if subtree.label() ==\"clause2b\":\n",
    "                #print(\"clause2b\")\n",
    "                #print(subtree.leaves())\n",
    "                review_lst.append(subtree.leaves())\n",
    "            if subtree.label() ==\"clause3\":\n",
    "                #print(\"clause3\")\n",
    "                #print(subtree.leaves())\n",
    "                review_lst.append(subtree.leaves())\n",
    "            if subtree.label() ==\"clause4\":\n",
    "                #print(\"clause4\")\n",
    "                #print(subtree.leaves())\n",
    "                review_lst.append(subtree.leaves())\n",
    "            if subtree.label() ==\"clause5\":\n",
    "                #print(\"clause5\")\n",
    "                #print(subtree.leaves())\n",
    "                review_lst.append(subtree.leaves())\n",
    "        \n",
    "    #Filtering out all the segments which do not have feature words              \n",
    "    review_lst2=[] #initialize a new list\n",
    "    for line in review_lst: #iterating the list with extracted segments\n",
    "        for item in line:\n",
    "            if item[0] in feature_lst: #Feature list is obtained in Step 3\n",
    "                review_lst2.append(line) #Append all the segments have feature words in the new list\n",
    "                continue\n",
    "        \n",
    "    end=time.time()\n",
    "    print(\"Time taken %f min\"%(((end-start)/ 60)))\n",
    "\n",
    "##################\n",
    "    #print(len(review_lst))\n",
    "    #print(review_lst)\n",
    "    print(\"# of segments\",len(review_lst2))\n",
    "    #print(review_lst2)\n",
    "##################\n",
    "    df_lst1=[]\n",
    "    df_lst2=[]\n",
    "\n",
    "    cnt=0\n",
    "    for tups in review_lst2:\n",
    "        lst=[]\n",
    "        tot_pscore=0\n",
    "        word_cnt=0\n",
    "        \n",
    "        for i in range(len(tups)):\n",
    "            tups[i]=list(tups[i]) #Convert the list back to tuples\n",
    "            w=tups[i][0].lower() #Lower the case of words and assign the word from the first element of each tuple\n",
    "            tag=tups[i][1] #Assign the tag from the 2nd element of tuples\n",
    "\n",
    "            #If the word is a negative contraction suffix ('negcontractions'), \n",
    "            #change it to the word: \"not\"\n",
    "            if w in negcontractions:\n",
    "                w=\"not\"\n",
    "                \n",
    "            # a word is a tuple (w, tag) such as ('bathrooms', 'NNS')\n",
    "            wntag = to_wntag(tag) # in WordNet, either noun/verb/adj/adv\n",
    "            if wntag is not None and (w not in stopwords or w in negwords): \n",
    "                #print(w)\n",
    "                tups[i][0] = w \n",
    "                word_cnt+=1\n",
    "                lemma = wn_lemmatize(w, wntag) #lemmatize token\n",
    "\n",
    "                if lemma is None:\n",
    "                    tups[i]=tuple(tups[i])\n",
    "                    continue\n",
    "\n",
    "                synsets = wn.synsets(lemma, pos=wntag) #all synsets of the word\n",
    "\n",
    "                if not synsets:\n",
    "                    tups[i]=tuple(tups[i])\n",
    "                    continue\n",
    "\n",
    "                # take the first sense\n",
    "                synset = synsets[0]\n",
    "                swn_synset = swn.senti_synset(synset.name()) #SentiWordNet synset\n",
    "                pscore = swn_synset.pos_score() - swn_synset.neg_score() #polarity score for the word\n",
    "                tot_pscore+=pscore\n",
    "\n",
    "                #Convert the list back to tuples\n",
    "                tups[i]=tuple(tups[i])\n",
    "\n",
    "            else:\n",
    "                tups[i][0] = w \n",
    "                tups[i]=tuple(tups[i])\n",
    "\n",
    "        df_lst1.append(tot_pscore/word_cnt)\n",
    "        print(\"df_lst1:\",df_lst1)\n",
    "\n",
    "\n",
    "        df_lst2.append(tups)\n",
    "    cnt+=1\n",
    "\n",
    "    \n",
    "    if np.mean(df_lst1) > 0 :\n",
    "        label=1\n",
    "    else:\n",
    "        label=0\n",
    "    #print(df_lst2)\n",
    "    avg_label_score_lst.append(np.mean(df_lst1))\n",
    "    label_lst.append(label)\n",
    "\n",
    "print(\"avg\",avg_label_score_lst)    \n",
    "print(\"label\",label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-6: Polarity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Given Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def to_wntag(pos):\n",
    "    \"\"\"\n",
    "    Given a Penn tag, returns the corresponding Wordnet pos tag.\n",
    "    Note that WordNet only contain nouns/verbs/adjectives/adverbs.\n",
    "    \"\"\"\n",
    "    ch = pos[0]\n",
    "    if ch == 'N':\n",
    "        return wn.NOUN #noun\n",
    "    elif ch == 'V':\n",
    "        return wn.VERB # verb\n",
    "    elif ch == 'J':\n",
    "        return wn.ADJ # adjective\n",
    "    elif ch == 'R':\n",
    "        return wn.ADV # adverb\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def wn_lemmatize(token, wntag):\n",
    "    \"\"\"\n",
    "    Applies WordNet morpher (which requires a pos) and \n",
    "    returns the morphological lemma form, or None.\n",
    "    \"\"\"\n",
    "    return wn.morphy(token, wntag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_lst2= [[('The', 'DT'), ('roOm', 'NNS'), ('is', 'VBP'), ('Not', 'RB'), ('HUGE', 'JJ')],\n",
    " #           [('the', 'DT'), ('rooms', 'NNS'), ('are', 'VBP'), ('very', 'RB'), ('Small', 'JJ')],\n",
    "  #         [('weird', 'JJ'), ('Windows', 'NNS')],\n",
    "   #       [('The', 'DT'), ('Beds', 'NNS'), ('were', 'VBD'), ('Very', 'RB'), ('good', 'JJ')],\n",
    "    #     [('the', 'DT'), ('Elevators', 'NNS'), ('are', 'VBZ'), ('Pretty', 'JJ')],\n",
    "     #   [('the', 'DT'), ('Elevator', 'NNS'), ('is', 'VBZ'), ('Pretty', 'JJ')],\n",
    "      #       [(\"Awesome\", 'NNP'),('Rooms', 'NNS'),('Comfy', 'VBP')]]\n",
    "#review_lst2 = [[('amenities', 'NNS'), ('were', 'VBD'), ('excellent', 'JJ'), ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=\"comfortable\"\n",
    "tag=\"JJ\"\n",
    "wntag = to_wntag(tag)\n",
    "lemma = wn_lemmatize(w, wntag)\n",
    "synsets = wn.synsets(lemma, pos=wntag)\n",
    "print(synsets)\n",
    "synset = synsets[2]\n",
    "\n",
    "\n",
    "swn_synset = swn.senti_synset(synset.name()) #SentiWordNet synset\n",
    "print(swn_synset)\n",
    "pscore = swn_synset.pos_score() - swn_synset.neg_score() #polarity score for the word\n",
    "pscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "negcontractions = [\"n't\", \"'t\"]\n",
    "negwords = ['not', 'no', \"nor\"]\n",
    "df_lst = [0 for item in range(len(review_lst2))]\n",
    "df_lst2 = []\n",
    "df_lst1 = []\n",
    "\n",
    "cnt=0\n",
    "for tups in review_lst2:\n",
    "    lst=[]\n",
    "    tot_pscore=0\n",
    "    word_cnt=0\n",
    "    for i in range(len(tups)):\n",
    "        tups[i]=list(tups[i]) #Convert the list back to tuples\n",
    "        w=tups[i][0].lower() #Lower the case of words and assign the word from the first element of each tuple\n",
    "        tag=tups[i][1] #Assign the tag from the 2nd element of tuples\n",
    "        \n",
    "        #If the word is a negative contraction suffix ('negcontractions'), \n",
    "        #change it to the word: \"not\"\n",
    "        if w in negcontractions:\n",
    "            w=\"not\"\n",
    "\n",
    "        # a word is a tuple (w, tag) such as ('bathrooms', 'NNS')\n",
    "        wntag = to_wntag(tag) # in WordNet, either noun/verb/adj/adv\n",
    "        if wntag is not None and (w not in stopwords or w in negwords): \n",
    "            #print(w)\n",
    "            tups[i][0] = w \n",
    "            word_cnt+=1\n",
    "            lemma = wn_lemmatize(w, wntag) #lemmatize token\n",
    "\n",
    "            if lemma is None:\n",
    "                tups[i]=tuple(tups[i])\n",
    "                continue\n",
    "\n",
    "            synsets = wn.synsets(lemma, pos=wntag) #all synsets of the word\n",
    "\n",
    "            if not synsets:\n",
    "                tups[i]=tuple(tups[i])\n",
    "                continue\n",
    "\n",
    "            # take the first sense\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name()) #SentiWordNet synset\n",
    "            pscore = swn_synset.pos_score() - swn_synset.neg_score() #polarity score for the word\n",
    "            tot_pscore+=pscore\n",
    "            \n",
    "            #Convert the list back to tuples\n",
    "            tups[i]=tuple(tups[i])\n",
    "\n",
    "        else:\n",
    "            tups[i][0] = w \n",
    "            tups[i]=tuple(tups[i])\n",
    "            \n",
    "    df_lst1.append(tot_pscore/word_cnt)\n",
    "    df_lst2.append(tups)\n",
    "    cnt+=1\n",
    "    \n",
    "    \n",
    "#Concatnate the tuples and Polarity Score in a DataFrame\n",
    "#df = pd.DataFrame({ i:pd.Series(value) for i, value in enumerate(df_lst) })\n",
    "df1= pd.DataFrame(df_lst1, columns= [\"Polarity_Score\"] )\n",
    "df2= pd.DataFrame(df_lst2)\n",
    "df = pd.concat((df1,df2), axis=1)\n",
    "df = df.sort_values(by=[\"Polarity_Score\",0,1,2,3,4,5,6,7,8,9,10], \n",
    "                    ascending=[False, True, True,True, True,True,True, True,True, True,True,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(review_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_lst=[]\n",
    "new_lst=[]\n",
    "cnt=0\n",
    "with open(\"XXXX-rooms.txt\", \"w\") as f:\n",
    "    for row in range(0,df.shape[0]):\n",
    "        new_lst = list(df.iloc[row])\n",
    "        new_lst = [item  for item in new_lst if item is not None]\n",
    "        #print(new_lst)\n",
    "\n",
    "        if pre_lst == new_lst:\n",
    "            continue\n",
    "            \n",
    "        for item in df.iloc[row]:\n",
    "            if item is not None:\n",
    "                try:\n",
    "                    if type(float(item)) == float:\n",
    "                        #print(item, end=\", [\")\n",
    "                        f.write(\"{0}, [\".format(item))\n",
    "                        continue\n",
    "                except:\n",
    "                    pass\n",
    "                #print(item, end=\",\")\n",
    "                f.write(\"{0}, \".format(item))\n",
    "\n",
    "            else:\n",
    "                #print(\"]\",end=\"\\n\")\n",
    "                f.write(\"]\\n\")\n",
    "                cnt+=1\n",
    "                break\n",
    "        pre_lst = new_lst\n",
    "f.close()\n",
    "print(cnt)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = nltk.pos_tag(nltk.word_tokenize(\"She girl let down her long hair\"))\n",
    "sent = nltk.pos_tag(nltk.word_tokenize(\"quick\"))\n",
    "print(sent)\n",
    "\n",
    "#regex grammar\n",
    "grammar = r\"\"\"\n",
    "NP:{<DT|PRP\\$>?<JJ>*<NN>} \n",
    "FOO:{<NP><VB.><JJ>}\"\"\"\n",
    "#\"NP:{DT|PRP\\$>?<JJ>*<NN>}\" #, FOO: {<NP><VB.><JJ>}\"\n",
    "\n",
    "#NLTK Regex parser\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(sent)\n",
    "print(tree)\n",
    "for subtree in tree.subtrees():\n",
    "    print(subtree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK Regex parser\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(sent)\n",
    "print(tree)\n",
    "for subtree in tree.subtrees():\n",
    "    print(subtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() ==\"NP\":\n",
    "        print(subtree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentlist:\n",
    "    cp.parse(nltk.pos_tag(sent))\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() ==\"NP\":\n",
    "            print(subtree.leaves())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
