{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/KevQuant/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"inaugural\")\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789-Washington.txt\n",
      "char 8619\n",
      "words 1538\n",
      "sents 24\n",
      "1793-Washington.txt\n",
      "char 791\n",
      "words 147\n",
      "sents 4\n",
      "1797-Adams.txt\n",
      "char 13877\n",
      "words 2585\n",
      "sents 37\n",
      "1801-Jefferson.txt\n",
      "char 10137\n",
      "words 1935\n",
      "sents 42\n",
      "1805-Jefferson.txt\n",
      "char 12908\n",
      "words 2384\n",
      "sents 45\n",
      "1809-Madison.txt\n",
      "char 7001\n",
      "words 1265\n",
      "sents 21\n",
      "1813-Madison.txt\n",
      "char 7157\n",
      "words 1304\n",
      "sents 33\n",
      "1817-Monroe.txt\n",
      "char 19887\n",
      "words 3693\n",
      "sents 122\n",
      "1821-Monroe.txt\n",
      "char 26326\n",
      "words 4909\n",
      "sents 129\n",
      "1825-Adams.txt\n",
      "char 17741\n",
      "words 3150\n",
      "sents 74\n",
      "1829-Jackson.txt\n",
      "char 6817\n",
      "words 1208\n",
      "sents 25\n",
      "1833-Jackson.txt\n",
      "char 7058\n",
      "words 1267\n",
      "sents 30\n",
      "1837-VanBuren.txt\n",
      "char 23417\n",
      "words 4171\n",
      "sents 95\n",
      "1841-Harrison.txt\n",
      "char 49700\n",
      "words 9165\n",
      "sents 210\n",
      "1845-Polk.txt\n",
      "char 28716\n",
      "words 5196\n",
      "sents 153\n",
      "1849-Taylor.txt\n",
      "char 6605\n",
      "words 1182\n",
      "sents 22\n",
      "1853-Pierce.txt\n",
      "char 20081\n",
      "words 3657\n",
      "sents 104\n",
      "1857-Buchanan.txt\n",
      "char 16815\n",
      "words 3098\n",
      "sents 89\n",
      "1861-Lincoln.txt\n",
      "char 21017\n",
      "words 4005\n",
      "sents 138\n",
      "1865-Lincoln.txt\n",
      "char 3926\n",
      "words 785\n",
      "sents 27\n",
      "1869-Grant.txt\n",
      "char 6503\n",
      "words 1239\n",
      "sents 41\n",
      "1873-Grant.txt\n",
      "char 7734\n",
      "words 1478\n",
      "sents 44\n",
      "1877-Hayes.txt\n",
      "char 14938\n",
      "words 2724\n",
      "sents 59\n",
      "1881-Garfield.txt\n",
      "char 17767\n",
      "words 3239\n",
      "sents 112\n",
      "1885-Cleveland.txt\n",
      "char 10145\n",
      "words 1828\n",
      "sents 44\n",
      "1889-Harrison.txt\n",
      "char 26179\n",
      "words 4750\n",
      "sents 157\n",
      "1893-Cleveland.txt\n",
      "char 12349\n",
      "words 2153\n",
      "sents 58\n",
      "1897-McKinley.txt\n",
      "char 23659\n",
      "words 4371\n",
      "sents 130\n",
      "1901-McKinley.txt\n",
      "char 13408\n",
      "words 2450\n",
      "sents 100\n",
      "1905-Roosevelt.txt\n",
      "char 5568\n",
      "words 1091\n",
      "sents 33\n",
      "1909-Taft.txt\n",
      "char 32164\n",
      "words 5846\n",
      "sents 159\n",
      "1913-Wilson.txt\n",
      "char 9563\n",
      "words 1905\n",
      "sents 68\n",
      "1917-Wilson.txt\n",
      "char 8395\n",
      "words 1656\n",
      "sents 60\n",
      "1921-Harding.txt\n",
      "char 20298\n",
      "words 3756\n",
      "sents 149\n",
      "1925-Coolidge.txt\n",
      "char 23949\n",
      "words 4442\n",
      "sents 197\n",
      "1929-Hoover.txt\n",
      "char 21764\n",
      "words 3890\n",
      "sents 158\n",
      "1933-Roosevelt.txt\n",
      "char 10903\n",
      "words 2063\n",
      "sents 85\n",
      "1937-Roosevelt.txt\n",
      "char 10607\n",
      "words 2019\n",
      "sents 96\n",
      "1941-Roosevelt.txt\n",
      "char 7571\n",
      "words 1536\n",
      "sents 68\n",
      "1945-Roosevelt.txt\n",
      "char 3039\n",
      "words 637\n",
      "sents 26\n",
      "1949-Truman.txt\n",
      "char 13679\n",
      "words 2528\n",
      "sents 116\n",
      "1953-Eisenhower.txt\n",
      "char 13955\n",
      "words 2775\n",
      "sents 123\n",
      "1957-Eisenhower.txt\n",
      "char 9190\n",
      "words 1917\n",
      "sents 92\n",
      "1961-Kennedy.txt\n",
      "char 7618\n",
      "words 1546\n",
      "sents 52\n",
      "1965-Johnson.txt\n",
      "char 8193\n",
      "words 1715\n",
      "sents 94\n",
      "1969-Nixon.txt\n",
      "char 11624\n",
      "words 2425\n",
      "sents 106\n",
      "1973-Nixon.txt\n",
      "char 9991\n",
      "words 2028\n",
      "sents 69\n",
      "1977-Carter.txt\n",
      "char 6873\n",
      "words 1380\n",
      "sents 53\n",
      "1981-Reagan.txt\n",
      "char 13735\n",
      "words 2801\n",
      "sents 127\n",
      "1985-Reagan.txt\n",
      "char 14561\n",
      "words 2946\n",
      "sents 126\n",
      "1989-Bush.txt\n",
      "char 12523\n",
      "words 2713\n",
      "sents 145\n",
      "1993-Clinton.txt\n",
      "char 9114\n",
      "words 1855\n",
      "sents 81\n",
      "1997-Clinton.txt\n",
      "char 12250\n",
      "words 2462\n",
      "sents 112\n",
      "2001-Bush.txt\n",
      "char 9053\n",
      "words 1825\n",
      "sents 97\n",
      "2005-Bush.txt\n",
      "char 12018\n",
      "words 2376\n",
      "sents 95\n",
      "2009-Obama.txt\n",
      "char 13439\n",
      "words 2726\n",
      "sents 112\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.fileids()\n",
    "\n",
    "for fileid in inaugural.fileids():\n",
    "    print(fileid)\n",
    "    print(\"char\",len(inaugural.raw(fileid)))\n",
    "    print(\"words\",len(inaugural.words(fileid)))\n",
    "    print(\"sents\",len(inaugural.sents(fileid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-1-1: Divide it into two datasets: preWWI (till 1913) and postWWI (after 1917)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preWWI_lst = [] #Create list to collect preWWI (till 1913) fileids\n",
    "postWWI_lst =[] #Create list to collect postWWI (after 1917) fileids\n",
    "\n",
    "for fileid in inaugural.fileids():\n",
    "    if int(fileid[0:4])<=1913: #Collect the fileids before or on 1913\n",
    "        preWWI_lst.append(fileid)\n",
    "    elif int(fileid[0:4]) >1917: #Collect the fileids after on 1917\n",
    "        postWWI_lst.append(fileid)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part-1-2: Normalize the text for each dataset by down-casing all words (but no stemming or lemmatization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89718\n",
      "38423\n"
     ]
    }
   ],
   "source": [
    "pre_words_lst = []\n",
    "for i in range(len(preWWI_lst)):\n",
    "    pre_words_lst.append(list(inaugural.words(preWWI_lst[i])))\n",
    "    \n",
    "pre_words_lst2 = []\n",
    "for i in range(len(pre_words_lst)):\n",
    "    pre_words_lst2 = pre_words_lst2 + pre_words_lst[i]    \n",
    "pre_words_lst2 = [word.lower() for word in pre_words_lst2]\n",
    "print(len(pre_words_lst2))\n",
    "\n",
    "pre_words_lst3 = []\n",
    "for word in pre_words_lst2:\n",
    "    if word in stopwords.words('english') or len(word)<=2:\n",
    "        pass\n",
    "    else:\n",
    "        pre_words_lst3.append(word)\n",
    "print(len(pre_words_lst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54361\n",
      "23063\n"
     ]
    }
   ],
   "source": [
    "post_words_lst = []\n",
    "for i in range(len(postWWI_lst)):\n",
    "    post_words_lst.append(list(inaugural.words(postWWI_lst[i])))\n",
    "    \n",
    "post_words_lst2 = []\n",
    "for i in range(len(post_words_lst)):\n",
    "    post_words_lst2 = post_words_lst2 + post_words_lst[i]    \n",
    "post_words_lst2 = [word.lower() for word in post_words_lst2]\n",
    "print(len(post_words_lst2))\n",
    "\n",
    "post_words_lst3 = []\n",
    "for word in post_words_lst2:\n",
    "    if word in stopwords.words('english') or len(word)<=2:\n",
    "        pass\n",
    "    else:\n",
    "        post_words_lst3.append(word)\n",
    "print(len(post_words_lst3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part-1-3: For each dataset, compute the frequency distribution (i.e., probability) of unigrams and bigrams.  Then compute the following and write two output files -- one for unigrams and one for bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     government 0.0047705\n",
      "         people 0.0039346\n",
      "         states 0.0031766\n",
      "           upon 0.0030317\n",
      "            may 0.0028311\n",
      "          shall 0.0025079\n",
      "          great 0.0024856\n",
      "        country 0.0024410\n",
      "          every 0.0021735\n",
      "         public 0.0021289\n",
      "   constitution 0.0021177\n",
      "       citizens 0.0019060\n",
      "          power 0.0018725\n",
      "          union 0.0018502\n",
      "          would 0.0016162\n",
      "         united 0.0015939\n",
      "           must 0.0015827\n",
      "            one 0.0015604\n",
      "         nation 0.0013487\n",
      "            war 0.0012929\n",
      "           made 0.0012261\n",
      "       congress 0.0012038\n",
      "          peace 0.0011926\n",
      "           laws 0.0011926\n",
      "         rights 0.0011703\n",
      "       national 0.0011480\n",
      "        without 0.0011480\n",
      "      interests 0.0011369\n",
      "         fellow 0.0011369\n",
      "           well 0.0011146\n",
      "           time 0.0010700\n",
      "          state 0.0010589\n",
      "        nations 0.0010477\n",
      "           free 0.0010366\n",
      "        foreign 0.0010143\n",
      "      executive 0.0010031\n",
      "            law 0.0009920\n",
      "         powers 0.0009920\n",
      "           duty 0.0009809\n",
      "           best 0.0009697\n",
      "           good 0.0009586\n",
      "         duties 0.0009363\n",
      "         policy 0.0009140\n",
      "         spirit 0.0009028\n",
      "          years 0.0008582\n",
      "            men 0.0008471\n",
      "          right 0.0008471\n",
      "      political 0.0008248\n",
      " administration 0.0008025\n",
      "          never 0.0007914\n",
      "          among 0.0007914\n",
      "       american 0.0007802\n",
      "        present 0.0007802\n",
      "      necessary 0.0007691\n",
      "        general 0.0007579\n",
      "     principles 0.0007579\n",
      "        justice 0.0007468\n",
      "   institutions 0.0007356\n",
      "          whole 0.0007245\n",
      "        support 0.0007245\n",
      "         within 0.0007245\n",
      "        liberty 0.0007133\n",
      "           make 0.0007133\n",
      "     confidence 0.0006911\n",
      "            far 0.0006911\n",
      "           part 0.0006799\n",
      "          world 0.0006688\n",
      "            new 0.0006688\n",
      "         proper 0.0006688\n",
      "           many 0.0006576\n",
      "           much 0.0006576\n",
      "        revenue 0.0006576\n",
      "       interest 0.0006465\n",
      "         system 0.0006465\n",
      "        subject 0.0006353\n",
      "          party 0.0006353\n",
      "         office 0.0006130\n",
      "           ever 0.0006130\n",
      "           less 0.0006019\n",
      "       commerce 0.0006019\n",
      "          might 0.0005907\n",
      "           high 0.0005907\n",
      "        service 0.0005907\n",
      "           long 0.0005907\n",
      "          equal 0.0005796\n",
      "     protection 0.0005796\n",
      "        federal 0.0005573\n",
      "          whose 0.0005573\n",
      "          first 0.0005573\n",
      "      principle 0.0005462\n",
      "          could 0.0005462\n",
      "           life 0.0005462\n",
      "          force 0.0005239\n",
      "          ought 0.0005239\n",
      "       question 0.0005239\n",
      "            yet 0.0005239\n",
      "         secure 0.0005239\n",
      "          civil 0.0005239\n",
      "         future 0.0005239\n",
      "      condition 0.0005127\n",
      "           hope 0.0005127\n",
      "      character 0.0005016\n",
      "     prosperity 0.0005016\n",
      "          trust 0.0005016\n",
      " constitutional 0.0005016\n",
      "        opinion 0.0005016\n",
      "       business 0.0005016\n",
      "        however 0.0004904\n",
      "       republic 0.0004904\n",
      "        respect 0.0004904\n",
      "      important 0.0004904\n",
      "      authority 0.0004904\n",
      "         others 0.0004793\n",
      "          found 0.0004793\n",
      "          since 0.0004793\n",
      "          still 0.0004793\n",
      "          given 0.0004793\n",
      "            let 0.0004793\n",
      "      influence 0.0004793\n",
      "         regard 0.0004793\n",
      "     countrymen 0.0004681\n",
      "           give 0.0004681\n",
      "          honor 0.0004681\n",
      "       domestic 0.0004570\n",
      "     experience 0.0004570\n",
      "         always 0.0004570\n",
      "     importance 0.0004570\n",
      "      therefore 0.0004458\n",
      "        freedom 0.0004458\n",
      "         course 0.0004458\n",
      "           take 0.0004347\n",
      "         called 0.0004347\n",
      "        parties 0.0004347\n",
      "      president 0.0004347\n",
      "          means 0.0004347\n",
      "           even 0.0004347\n",
      "    legislation 0.0004235\n",
      "           full 0.0004235\n",
      "        citizen 0.0004235\n",
      "  circumstances 0.0004235\n",
      "         common 0.0004235\n",
      "          order 0.0004235\n",
      "       measures 0.0004124\n",
      "      happiness 0.0004124\n",
      "            man 0.0004124\n",
      "           true 0.0004124\n",
      "        control 0.0004013\n",
      "         either 0.0004013\n",
      "       purposes 0.0004013\n",
      "    governments 0.0004013\n",
      "         object 0.0004013\n",
      "      relations 0.0004013\n",
      "          sense 0.0004013\n",
      "        purpose 0.0004013\n",
      "       exercise 0.0003901\n",
      "      blessings 0.0003901\n",
      "         become 0.0003901\n",
      "      territory 0.0003901\n",
      "       progress 0.0003901\n",
      "          trade 0.0003901\n",
      "          place 0.0003790\n",
      "           also 0.0003790\n",
      "          labor 0.0003790\n",
      "       preserve 0.0003790\n",
      "      attention 0.0003678\n",
      "       maintain 0.0003678\n",
      "            aid 0.0003678\n",
      "         effect 0.0003678\n",
      "          cause 0.0003678\n",
      "          local 0.0003567\n",
      "         danger 0.0003567\n",
      "        history 0.0003567\n",
      "        whether 0.0003567\n",
      "           form 0.0003567\n",
      "        nothing 0.0003567\n",
      "         limits 0.0003567\n",
      "         action 0.0003567\n",
      "          favor 0.0003567\n",
      "           like 0.0003567\n",
      "            day 0.0003567\n",
      "         safety 0.0003455\n",
      "         better 0.0003455\n",
      "         wisdom 0.0003455\n",
      "         period 0.0003455\n",
      "       faithful 0.0003455\n",
      "           find 0.0003455\n",
      "           home 0.0003455\n",
      "          hands 0.0003455\n",
      "          human 0.0003455\n",
      "           past 0.0003455\n",
      "     population 0.0003344\n",
      "        affairs 0.0003344\n",
      "       judgment 0.0003344\n",
      "           self 0.0003344\n",
      "        another 0.0003344\n",
      "       possible 0.0003344\n",
      "           mind 0.0003344\n",
      "         desire 0.0003344\n",
      "        objects 0.0003344\n",
      "           look 0.0003232\n",
      "        defense 0.0003232\n",
      "     patriotism 0.0003232\n",
      "           work 0.0003232\n",
      "           view 0.0003232\n",
      "           none 0.0003232\n",
      "         tariff 0.0003232\n",
      "          large 0.0003232\n",
      "       occasion 0.0003232\n",
      "      existence 0.0003232\n",
      "       increase 0.0003232\n",
      "        believe 0.0003232\n",
      "          south 0.0003232\n",
      "           wise 0.0003121\n",
      "            two 0.0003121\n",
      "           thus 0.0003121\n",
      "      discharge 0.0003121\n",
      "     individual 0.0003121\n",
      "           land 0.0003121\n",
      "      questions 0.0003121\n",
      "           body 0.0003121\n",
      "           fair 0.0003121\n",
      "         change 0.0003009\n",
      "         extent 0.0003009\n",
      "      resources 0.0003009\n",
      "     sufficient 0.0003009\n",
      "       continue 0.0003009\n",
      "          happy 0.0003009\n",
      "         reason 0.0003009\n",
      "       security 0.0003009\n",
      "  consideration 0.0003009\n",
      "           care 0.0003009\n",
      "       strength 0.0003009\n",
      "        success 0.0003009\n",
      "           oath 0.0003009\n",
      "        example 0.0003009\n",
      "        protect 0.0003009\n",
      "        portion 0.0003009\n",
      "        conduct 0.0003009\n",
      "          faith 0.0003009\n",
      "           love 0.0003009\n",
      "         things 0.0003009\n",
      "        several 0.0002898\n",
      "           race 0.0002898\n",
      "       extended 0.0002898\n",
      "         passed 0.0002898\n",
      "         degree 0.0002898\n",
      "       opinions 0.0002898\n",
      "    independent 0.0002898\n",
      "      necessity 0.0002898\n",
      "        greater 0.0002898\n",
      "           hand 0.0002898\n",
      "          times 0.0002898\n",
      "           come 0.0002787\n",
      "         toward 0.0002787\n",
      "         result 0.0002787\n",
      "        welfare 0.0002787\n",
      "      patriotic 0.0002787\n",
      "          money 0.0002787\n",
      "        highest 0.0002787\n",
      "        members 0.0002787\n",
      "            god 0.0002787\n",
      "    departments 0.0002787\n",
      "         placed 0.0002787\n",
      "            act 0.0002787\n",
      "           case 0.0002787\n",
      "         nature 0.0002787\n",
      "          views 0.0002787\n",
      "        certain 0.0002787\n",
      "        promote 0.0002675\n",
      "        produce 0.0002675\n",
      "            use 0.0002675\n",
      "           meet 0.0002675\n",
      "    intercourse 0.0002675\n",
      "     throughout 0.0002675\n",
      "     instrument 0.0002675\n",
      "      essential 0.0002675\n",
      "         formed 0.0002564\n",
      "            due 0.0002564\n",
      "    established 0.0002564\n",
      "           need 0.0002564\n",
      "          chief 0.0002564\n",
      "           debt 0.0002564\n",
      "        settled 0.0002564\n",
      "        equally 0.0002564\n",
      "        harmony 0.0002564\n",
      "        already 0.0002564\n",
      "       industry 0.0002564\n",
      "           feel 0.0002564\n",
      "       position 0.0002564\n",
      "      honorable 0.0002564\n",
      "            see 0.0002564\n",
      "       election 0.0002564\n",
      "        economy 0.0002564\n",
      "            way 0.0002564\n",
      "    obligations 0.0002564\n",
      "         beyond 0.0002564\n",
      "           done 0.0002452\n",
      "   independence 0.0002452\n",
      "        require 0.0002452\n",
      "         solemn 0.0002452\n",
      "        thought 0.0002452\n",
      "      religious 0.0002452\n",
      "    improvement 0.0002452\n",
      "     revolution 0.0002452\n",
      "         expect 0.0002452\n",
      "     department 0.0002452\n",
      "         effort 0.0002452\n",
      "         honest 0.0002452\n",
      "       officers 0.0002452\n",
      "        ability 0.0002452\n",
      "   expenditures 0.0002452\n",
      "       currency 0.0002341\n",
      "      countries 0.0002341\n",
      "  distinguished 0.0002341\n",
      "          taken 0.0002341\n",
      "        feeling 0.0002341\n",
      "           navy 0.0002341\n",
      "       whatever 0.0002341\n",
      "       property 0.0002341\n",
      "        slavery 0.0002341\n",
      "       powerful 0.0002341\n",
      "       endeavor 0.0002341\n",
      "        efforts 0.0002341\n",
      "          parts 0.0002341\n",
      "        neither 0.0002341\n",
      "        prevent 0.0002341\n",
      "    legislative 0.0002341\n",
      "       military 0.0002341\n",
      "         indeed 0.0002341\n",
      "    sovereignty 0.0002229\n",
      "           vast 0.0002229\n",
      "       presence 0.0002229\n",
      "       internal 0.0002229\n",
      "      preserved 0.0002229\n",
      "           last 0.0002229\n",
      "       personal 0.0002229\n",
      "         making 0.0002229\n",
      "          early 0.0002229\n",
      "     providence 0.0002229\n",
      "           rule 0.0002229\n",
      "          terms 0.0002229\n",
      "     obligation 0.0002229\n",
      "        fathers 0.0002229\n",
      "      permanent 0.0002229\n",
      "        perfect 0.0002229\n",
      "          moral 0.0002229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     conditions 0.0002229\n",
      " responsibility 0.0002229\n",
      "       friendly 0.0002229\n",
      "            say 0.0002229\n",
      "    confederacy 0.0002118\n",
      "         strong 0.0002118\n",
      "          value 0.0002118\n",
      "          alone 0.0002118\n",
      "    individuals 0.0002118\n"
     ]
    }
   ],
   "source": [
    "fduni = nltk.FreqDist(pre_words_lst3)\n",
    "pre_df = pd.DataFrame(list(zip(fduni.keys(), fduni.values())), columns =[\"tokens\", \"freq\"])\n",
    "pre_df = pre_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "\n",
    "pre_df[\"pre_prob\"] = [round(freq / len(pre_words_lst2),7) for freq in pre_df.freq]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pre_df = pre_df.sort_values(by = [\"pre_prob\"], ascending = False)\n",
    "pre_df = pre_df.iloc[0:355]\n",
    "for i in range(0, pre_df.shape[0]):\n",
    "    print(\"%15s %0.7f\"%(pre_df.iloc[i][\"tokens\"], pre_df.iloc[i][\"pre_prob\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          world 0.0048021\n",
      "         people 0.0037489\n",
      "           must 0.0036418\n",
      "         nation 0.0032312\n",
      "            new 0.0032133\n",
      "        america 0.0031598\n",
      "     government 0.0029455\n",
      "          peace 0.0025885\n",
      "        freedom 0.0025528\n",
      "           time 0.0019637\n",
      "          great 0.0019280\n",
      "            let 0.0018923\n",
      "            one 0.0018387\n",
      "        nations 0.0017852\n",
      "           upon 0.0017316\n",
      "          every 0.0016067\n",
      "          shall 0.0015888\n",
      "           know 0.0015531\n",
      "           free 0.0015352\n",
      "          today 0.0015174\n",
      "           life 0.0014995\n",
      "           work 0.0014995\n",
      "        country 0.0014817\n",
      "            may 0.0014281\n",
      "       american 0.0013746\n",
      "            god 0.0013032\n",
      "           make 0.0012318\n",
      "           hope 0.0012318\n",
      "            men 0.0012139\n",
      "        justice 0.0011961\n",
      "       citizens 0.0011782\n",
      "      americans 0.0011604\n",
      "            man 0.0011604\n",
      "          would 0.0011425\n",
      "        history 0.0011425\n",
      "          power 0.0011068\n",
      "          human 0.0011068\n",
      "            old 0.0010890\n",
      "       together 0.0010711\n",
      "          faith 0.0010711\n",
      "            war 0.0010533\n",
      "       progress 0.0010354\n",
      "           good 0.0010175\n",
      "          years 0.0010175\n",
      "         spirit 0.0010175\n",
      "            day 0.0009997\n",
      "            way 0.0009818\n",
      "         united 0.0009640\n",
      "          never 0.0009640\n",
      "      democracy 0.0009461\n",
      "        liberty 0.0009283\n",
      "           help 0.0009283\n",
      "       national 0.0009104\n",
      "           need 0.0008926\n",
      "           come 0.0008926\n",
      "         common 0.0008747\n",
      "        purpose 0.0008569\n",
      "         fellow 0.0008212\n",
      "            law 0.0008033\n",
      "          first 0.0008033\n",
      "       strength 0.0008033\n",
      "         better 0.0007855\n",
      "          earth 0.0007855\n",
      "           live 0.0007855\n",
      "         states 0.0007855\n",
      "      president 0.0007676\n",
      " responsibility 0.0007676\n",
      "        century 0.0007676\n",
      "           long 0.0007676\n",
      "         future 0.0007498\n",
      "           many 0.0007498\n",
      "         things 0.0007319\n",
      "           made 0.0007319\n",
      "           land 0.0007319\n",
      "        believe 0.0007319\n",
      "       economic 0.0007319\n",
      "         cannot 0.0007141\n",
      "           seek 0.0007141\n",
      "           much 0.0007141\n",
      "        peoples 0.0006962\n",
      "    opportunity 0.0006962\n",
      "         change 0.0006962\n",
      "          stand 0.0006784\n",
      "           home 0.0006784\n",
      "           well 0.0006605\n",
      "            see 0.0006605\n",
      "          right 0.0006605\n",
      "          place 0.0006427\n",
      "          among 0.0006427\n",
      "          lives 0.0006427\n",
      "       security 0.0006248\n",
      "           find 0.0006248\n",
      "        without 0.0006248\n",
      "           part 0.0006070\n",
      "            yet 0.0006070\n",
      "         always 0.0006070\n",
      "           done 0.0006070\n",
      "         public 0.0006070\n",
      "         strong 0.0006070\n",
      "         toward 0.0006070\n",
      "         beyond 0.0006070\n",
      "         action 0.0006070\n",
      "           best 0.0005891\n",
      "        promise 0.0005891\n",
      "      political 0.0005713\n",
      "          still 0.0005713\n",
      "          force 0.0005713\n",
      "         others 0.0005713\n",
      "           self 0.0005713\n",
      "           ever 0.0005534\n",
      "         system 0.0005534\n",
      "       children 0.0005534\n",
      "         rights 0.0005534\n",
      "           less 0.0005534\n",
      "          means 0.0005534\n",
      "        service 0.0005534\n",
      "            ask 0.0005356\n",
      "           even 0.0005356\n",
      "            end 0.0005356\n",
      "            use 0.0005177\n",
      "        economy 0.0005177\n",
      "          heart 0.0005177\n",
      "           meet 0.0004998\n",
      "     generation 0.0004998\n",
      "        mankind 0.0004998\n",
      "           duty 0.0004998\n",
      "          order 0.0004998\n",
      "         within 0.0004998\n",
      "         ideals 0.0004820\n",
      "           full 0.0004820\n",
      "        greater 0.0004820\n",
      "        courage 0.0004820\n",
      "       millions 0.0004641\n",
      "          alone 0.0004641\n",
      "       problems 0.0004641\n",
      "        forward 0.0004641\n",
      "   civilization 0.0004641\n",
      "           task 0.0004641\n",
      "          unity 0.0004641\n",
      "         moment 0.0004641\n",
      "           like 0.0004641\n",
      "        another 0.0004641\n",
      "          whole 0.0004463\n",
      "     principles 0.0004463\n",
      "          cause 0.0004463\n",
      "     confidence 0.0004463\n",
      "           give 0.0004463\n",
      "          moral 0.0004463\n",
      "            act 0.0004463\n",
      "           high 0.0004463\n",
      "        friends 0.0004284\n",
      "        support 0.0004284\n",
      "          times 0.0004284\n",
      "            far 0.0004284\n",
      "           take 0.0004284\n",
      "           true 0.0004284\n",
      "        dignity 0.0004284\n",
      "      resources 0.0004284\n",
      "           last 0.0004284\n",
      "          women 0.0004284\n",
      "        society 0.0004106\n",
      "           laws 0.0004106\n",
      "         pledge 0.0004106\n",
      "         called 0.0004106\n",
      "          small 0.0004106\n",
      "          words 0.0004106\n",
      "     prosperity 0.0004106\n",
      "           face 0.0004106\n",
      "           hand 0.0004106\n",
      "       continue 0.0004106\n",
      "         effort 0.0004106\n",
      "           fear 0.0004106\n",
      "          bring 0.0004106\n",
      "          given 0.0004106\n",
      "           love 0.0004106\n",
      "       republic 0.0003927\n",
      "           oath 0.0003927\n",
      "           hold 0.0003927\n",
      "            ago 0.0003927\n",
      "   independence 0.0003927\n",
      "          union 0.0003927\n",
      "          party 0.0003927\n",
      "     individual 0.0003927\n",
      "        destiny 0.0003927\n",
      "         become 0.0003749\n",
      "       congress 0.0003749\n",
      "           past 0.0003749\n",
      "           call 0.0003749\n",
      "  international 0.0003749\n",
      "        federal 0.0003749\n",
      "      knowledge 0.0003749\n",
      "           back 0.0003749\n",
      "          speak 0.0003570\n",
      "      beginning 0.0003570\n",
      "            say 0.0003570\n",
      "         secure 0.0003570\n",
      "     leadership 0.0003570\n",
      "           want 0.0003570\n",
      "            era 0.0003570\n",
      "          could 0.0003570\n",
      "           four 0.0003570\n",
      "          found 0.0003570\n",
      "       greatest 0.0003570\n",
      "        advance 0.0003570\n",
      "          build 0.0003570\n",
      "      sacrifice 0.0003392\n",
      "      important 0.0003392\n",
      "          clear 0.0003392\n",
      "          equal 0.0003392\n",
      "           also 0.0003392\n",
      "         across 0.0003392\n",
      "        defense 0.0003392\n",
      "          share 0.0003392\n",
      "           lead 0.0003392\n",
      "          taken 0.0003392\n",
      "        nothing 0.0003213\n",
      "          hopes 0.0003213\n",
      "    generations 0.0003213\n",
      "         enough 0.0003213\n",
      "        success 0.0003213\n",
      "        concern 0.0003213\n",
      "       business 0.0003213\n",
      "     industrial 0.0003213\n",
      "  understanding 0.0003213\n",
      "       interest 0.0003213\n",
      "          young 0.0003213\n",
      " administration 0.0003213\n",
      "         office 0.0003213\n",
      "        poverty 0.0003213\n",
      "         making 0.0003213\n",
      "         social 0.0003035\n",
      "         values 0.0003035\n",
      "           wish 0.0003035\n",
      "      education 0.0003035\n",
      "          ready 0.0003035\n",
      "         abroad 0.0003035\n",
      "            two 0.0003035\n",
      "          honor 0.0003035\n",
      "         wealth 0.0003035\n",
      "        thought 0.0003035\n",
      "         vision 0.0003035\n",
      "          light 0.0003035\n",
      "          since 0.0003035\n",
      "        resolve 0.0003035\n",
      "           join 0.0003035\n",
      "          dream 0.0003035\n",
      "         belief 0.0003035\n",
      "         forces 0.0002856\n",
      "          sense 0.0002856\n",
      "            age 0.0002856\n",
      "          serve 0.0002856\n",
      "          whose 0.0002856\n",
      "    cooperation 0.0002856\n",
      "        present 0.0002856\n",
      "           hard 0.0002856\n",
      "          reach 0.0002856\n",
      "        welfare 0.0002856\n",
      "           seen 0.0002856\n",
      "           days 0.0002856\n",
      "           arms 0.0002856\n",
      "           came 0.0002856\n",
      "        efforts 0.0002856\n",
      "        citizen 0.0002856\n",
      "           turn 0.0002856\n",
      "           lies 0.0002856\n",
      "     expression 0.0002856\n",
      "       humanity 0.0002856\n",
      "     determined 0.0002856\n",
      "       remember 0.0002856\n",
      "     conscience 0.0002856\n",
      "        achieve 0.0002678\n",
      "          trade 0.0002678\n",
      "      challenge 0.0002678\n",
      "           away 0.0002678\n",
      "         policy 0.0002678\n",
      "        special 0.0002678\n",
      "          might 0.0002678\n",
      "         chance 0.0002678\n",
      "          trust 0.0002678\n",
      "       building 0.0002678\n",
      "           year 0.0002678\n",
      "         voices 0.0002678\n",
      "         accept 0.0002678\n",
      "      standards 0.0002678\n",
      "   constitution 0.0002678\n",
      "         little 0.0002678\n",
      "         strive 0.0002678\n",
      "         course 0.0002678\n",
      "            set 0.0002678\n",
      "    governments 0.0002678\n",
      "        whether 0.0002678\n",
      "           fail 0.0002678\n",
      "        journey 0.0002678\n",
      "      essential 0.0002678\n",
      "      community 0.0002678\n",
      "          think 0.0002678\n",
      "          story 0.0002678\n",
      "           race 0.0002678\n",
      "        respect 0.0002678\n",
      "         simple 0.0002678\n",
      "          thank 0.0002678\n",
      "     conditions 0.0002499\n",
      "            tax 0.0002499\n",
      "          hands 0.0002499\n",
      "          bless 0.0002499\n",
      "      influence 0.0002499\n",
      "          truth 0.0002499\n",
      "         longer 0.0002499\n",
      "         living 0.0002499\n",
      "          sound 0.0002499\n",
      "responsibilities 0.0002499\n",
      "           fact 0.0002499\n",
      "       peaceful 0.0002499\n",
      "           look 0.0002499\n",
      "          state 0.0002499\n",
      "           cost 0.0002499\n",
      "       policies 0.0002499\n",
      "           rule 0.0002499\n",
      "     challenges 0.0002499\n",
      "     throughout 0.0002499\n",
      "        instead 0.0002499\n",
      "        weapons 0.0002499\n",
      "           vice 0.0002499\n",
      "         though 0.0002499\n",
      "         hearts 0.0002499\n",
      "        program 0.0002499\n",
      "         helped 0.0002499\n",
      "      character 0.0002499\n",
      "      permanent 0.0002499\n",
      "         wisdom 0.0002499\n",
      "         growth 0.0002499\n",
      "      countries 0.0002321\n",
      "          labor 0.0002321\n",
      "        highest 0.0002321\n",
      "           born 0.0002321\n",
      "       purposes 0.0002321\n",
      "      necessary 0.0002321\n",
      "       maintain 0.0002321\n",
      "          renew 0.0002321\n",
      "     revolution 0.0002321\n",
      "           care 0.0002321\n",
      "     countrymen 0.0002321\n",
      "          ought 0.0002321\n",
      "        seeking 0.0002321\n",
      "          forth 0.0002321\n",
      "        control 0.0002321\n",
      "      centuries 0.0002321\n",
      "         desire 0.0002321\n",
      "         choice 0.0002321\n",
      "         merely 0.0002321\n",
      "          carry 0.0002321\n",
      "      something 0.0002321\n",
      "       material 0.0002321\n",
      "        provide 0.0002142\n",
      "      different 0.0002142\n"
     ]
    }
   ],
   "source": [
    "fduni = nltk.FreqDist(post_words_lst3)\n",
    "post_df = pd.DataFrame(list(zip(fduni.keys(), fduni.values())), columns =[\"tokens\", \"freq\"])\n",
    "post_df = post_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "\n",
    "post_df[\"post_prob\"] = [round(freq / len(post_words_lst2),7) for freq in post_df.freq]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "post_df = post_df.sort_values(by = [\"post_prob\"], ascending = False)\n",
    "\n",
    "post_df = post_df.iloc[0:355]\n",
    "#post_df\n",
    "for i in range(0, post_df.shape[0]):\n",
    "    print(\"%15s %0.7f\"%(post_df.iloc[i][\"tokens\"], post_df.iloc[i][\"post_prob\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\")\n",
    "pre_post_df = pre_post_df.sort_values(by=[\"tokens\"])\n",
    "pre_post_df.drop(columns=[\"freq_x\", \"freq_y\"], inplace=True)\n",
    "pre_post_df.to_csv(\"unigrams.txt\", index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_bigrams = list(nltk.bigrams(pre_words_lst2))\n",
    "fdbi = nltk.FreqDist(pre_bigrams)\n",
    "pre_df = pd.DataFrame(list(zip(fdbi.keys(), fdbi.values())), columns =[\"tokens\", \"freq\"])\n",
    "pre_df = pre_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "\n",
    "pre_df[\"pre_prob\"] = [round(freq / len(pre_words_lst2),7) for freq in pre_df.freq]\n",
    "\n",
    "\n",
    "pre_df = pre_df.sort_values(by = [\"pre_prob\"], ascending = False)\n",
    "#pre_df = pre_df.iloc[0:30000]\n",
    "#for i in range(0, pre_df.shape[0]):\n",
    " #   print(\"%15s %0.7f\"%(pre_df.iloc[i][\"tokens\"], pre_df.iloc[i][\"pre_prob\"]))\n",
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fellow'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_bigrams[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_bigrams = list(nltk.bigrams(post_words_lst2))\n",
    "fdbi = nltk.FreqDist(post_bigrams)\n",
    "post_df = pd.DataFrame(list(zip(fdbi.keys(), fdbi.values())), columns =[\"tokens\", \"freq\"])\n",
    "post_df = post_df.sort_values(by=[\"freq\"], ascending=False)\n",
    "\n",
    "post_df[\"post_prob\"] = [round(freq / len(post_words_lst2),7) for freq in post_df.freq]\n",
    "\n",
    "\n",
    "post_df = post_df.sort_values(by = [\"post_prob\"], ascending = False)\n",
    "#post_df = post_df.iloc[0:30000]\n",
    "#for i in range(0, post_df.shape[0]):\n",
    " #   print(\"%15s %0.7f\"%(post_df.iloc[i][\"tokens\"], post_df.iloc[i][\"post_prob\"]))\n",
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_words_lst3 = []\n",
    "for word in pre_words_lst2:\n",
    "    if word in stopwords.words('english') or len(word)<=2:\n",
    "        pass\n",
    "    else:\n",
    "        pre_words_lst3.append(word)\n",
    "print(len(pre_words_lst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>pre_prob</th>\n",
       "      <th>post_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>(\", and)</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>(\", i)</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>(\", in)</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5439</th>\n",
       "      <td>(\", is)</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>(\", let)</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>(\", our)</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>(\", that)</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>(\", the)</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>(\", their)</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>(\", to)</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens  pre_prob  post_prob\n",
       "2067    (\", and)  0.000045   0.000074\n",
       "4325      (\", i)  0.000022   0.000018\n",
       "2274     (\", in)  0.000045   0.000037\n",
       "5439     (\", is)  0.000011   0.000018\n",
       "7771    (\", let)  0.000011   0.000018\n",
       "5679    (\", our)  0.000011   0.000018\n",
       "6005   (\", that)  0.000011   0.000018\n",
       "690     (\", the)  0.000123   0.000055\n",
       "2722  (\", their)  0.000033   0.000018\n",
       "694      (\", to)  0.000123   0.000018"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\")\n",
    "pre_post_df = pre_post_df.sort_values(by=[\"tokens\"])\n",
    "pre_post_df.drop(columns=[\"freq_x\", \"freq_y\"], inplace=True)\n",
    "pre_post_df[0:10]\n",
    "#pre_post_df.to_csv(\"unigrams.txt\", index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_df.set_index(\"tokens\").join(post_df.set_index(\"tokens\"),)\n",
    "pre_post_df = pd.merge(pre_df, post_df, on=\"tokens\")\n",
    "pre_post_df.columns = [\"unigram\", \"pre-WWI\",\"post-WWI\" ]\n",
    "pre_post_df = pre_post_df.sort_values(by = [\"pre-WWI\"], ascending=False)\n",
    "pre_post_df = pre_post_df[0:200]\n",
    "pre_post_df.sort_values(by = [\"unigram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating the preWWI list to down-casing all words\n",
    "for i in range(len(preWWI_lst)):\n",
    "    preWWI_lst[i] = inaugural.raw(preWWI_lst[i]).lower() #Store the preWWI data (ALL words) to the same list preWWI_lst\n",
    "\n",
    "#Iterating the postWWI_lst list to down-casing all words\n",
    "for i in range(len(postWWI_lst)):\n",
    "    postWWI_lst[i] = inaugural.raw(postWWI_lst[i]).lower() #Store the postWWI data (ALL words) to the same list postWWI_lst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part-1-3: For each dataset, compute the frequency distribution (i.e., probability) of unigrams and bigrams.  Then compute the following and write two output files -- one for unigrams and one for bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/KevQuant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c69ce2ae1974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwords_lst3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_lst2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "words_lst=[]\n",
    "for i in range(len(preWWI_lst)):\n",
    "    words_lst.append( word_tokenize(preWWI_lst[i]))\n",
    "\n",
    "words_lst2=[]\n",
    "for i in range(len(words_lst)):\n",
    "    words_lst2 +=words_lst[i]\n",
    "\n",
    "words_lst3=[]\n",
    "for word in words_lst2:\n",
    "    if (len(word)<=2) or (word in stopwords.words()):\n",
    "        pass\n",
    "    else:\n",
    "        words_lst3.append(word)\n",
    "\n",
    "fduni = nltk.FreqDist(words_lst3)\n",
    "len(fduni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preWWI_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6769"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict = {}\n",
    "for words in preWWI_lst:\n",
    "    words = word_tokenize(words)\n",
    "    for word in words:\n",
    "        if word in stopwords.words() or len(word)<=2:\n",
    "            pass\n",
    "        else:\n",
    "            if word.strip() in words_dict.keys():\n",
    "                words_dict[word.strip()]+=1\n",
    "            else:\n",
    "                words_dict[word.strip()]=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6761"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict2 = {}\n",
    "for k,v in words_dict.items():\n",
    "    if v >= 200:\n",
    "        pass\n",
    "    else:\n",
    "        words_dict2[k] = v\n",
    "words_dict2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(words_dict2.keys(), words_dict2.values())), columns =[\"tokens\", \"freq\"])\n",
    "df.sort_values(by=[\"tokens\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-01ea01abaaf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreWWI_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreWWI_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreWWI_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreWWI_words\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mpreWWI_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreWWI_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfreq_uni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "preWWI_words=[]\n",
    "for i in range(len(preWWI_lst)):\n",
    "    preWWI_words = preWWI_words +preWWI_lst[i]\n",
    "words = word_tokenize(preWWI_words)\n",
    "freq_uni = nltk.FreqDist(words)\n",
    "df = pd.DataFrame(list(zip(freq_uni.keys(), freq_uni.values())), columns = [\"tokens\", \"freq\"])\n",
    "#df.drop(labels=[5])\n",
    "df.sort_values(by = [\"tokens\"])\n",
    "#df.drop(labels=[5])\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preWWI_words = []\n",
    "for i in range(len(preWWI_lst)):\n",
    "    preWWI_words = preWWI_words  + [preWWI_lst[i]]\n",
    "\n",
    "len(preWWI_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488573"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt =0\n",
    "for words in preWWI_lst:\n",
    "    cnt += len(words)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abced']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = [[\"abced\"], [\"fuahf fiaha\"]]\n",
    "\n",
    "aa= aa[0]+aa[1]\n",
    "word_tokenize(aa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6936"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst=[]\n",
    "for i in range(len(preWWI_lst)):\n",
    "    words_lst.append( word_tokenize(preWWI_lst[i]))\n",
    "\n",
    "words_lst2=[]\n",
    "for i in range(len(words_lst)):\n",
    "    words_lst2 +=words_lst[i]\n",
    "\n",
    "fduni = nltk.FreqDist(words_lst2)\n",
    "len(fduni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89345"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt=1\n",
    "for words in words_lst:\n",
    "    cnt += len(words)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams: 145735\n",
      "bigram 145734\n"
     ]
    }
   ],
   "source": [
    "unigrams = inaugural.words()\n",
    "print(\"unigrams:\", len(unigrams))\n",
    "bigrams = list(nltk.bigrams(unigrams))\n",
    "print(\"bigram\",len(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fduni = nltk.FreqDist(unigrams)\n",
    "print(\"length of unigrams Dict\",len(fduni))\n",
    "\n",
    "for (w,c) in fduni.items():\n",
    "    print(w,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Check out the frequency of each word link to another word next to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('-', 7), ('citizens', 16), ('Citizens', 1)])\n"
     ]
    }
   ],
   "source": [
    "cfd =  nltk.ConditionalFreqDist(bigrams)\n",
    "print(cfd[\"Fellow\"].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Conditional Probabilities of \n",
    "##(freq(\"-\") + freq(\"Fellow\")) divided by (Total freq(\"fellow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2916666666666667"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"Fellow\"].freq(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
